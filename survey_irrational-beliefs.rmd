---
title: "survey_irrational-beliefs"
output: word_document
bibliography: library.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, comment = NA,
                      fig.height = 15, fig.width = 15)
#this chunk is used to silence code, warnings, comments and hash from appearing in output.
```


# Statistical analysis 


R packages used in the analysis were psych version 1.8.4 for factor analysis , HH version 3.1-35 for plotting likert data, factorMerger version 0.3.6 for merging factor levels and broom version 0.5.0 for tidying up model coefficients. 


# Results



```{r}
options(scipen = 999)
options("mc.cores"=2) 

library(data.table)
library(lavaan)
library(psych)
library(factorMerger) # for post-hoc analysis for MANOVA
library(mvoutlier) # detecting multivariate outliers
library(micompr) # for testing for normality and homogenity of variance in MANOVA
library(mvnormtest)  # for shapiro test of normality for MANOVA
library(table1)
library(tidyverse)
library(FactoMineR)
library(factoextra)
library(tibble)
library(corpcor)
library(GPArotation)
library(psych)
library(stargazer)
library(broom)
library(ggpubr)
require(foreign)
require(ggplot2)
require(MASS)
require(Hmisc)
require(reshape2)
library(knitr)
library(HH) # for plotting likert data
library(citr) # for citation
library(flextable) # nice tables
library(corrr) # for correlation matrix, a really nice package
library(ICSNP)   # for hotelling test
library(readxl)
library(mirt) # for fitting multidimentional IRT 
```


```{r}
# When you are done writing, your bibliography file is likely to contain some unneeded references, which you added while writing but removed during revisions. tidy_bib_file() removes unneeded (or duplicate) entries from your bibliography file.

SurveyData <-read.csv("es_data.csv")

# ------------------------- primary data checks --------------------------------
## complete cases Questions
completeSurveyData=SurveyData[complete.cases(SurveyData[2:62]), ]

## complete cases all
completeSurveyData=SurveyData[complete.cases(SurveyData), ]

gov_total <- read_csv("HamzaGovern.csv")
# correction for some MRNs mistyped 

fir <- str_extract(SurveyData$MRN, "\\d{4}")   

las <- substring(SurveyData$MRN, 5) 

las <- str_pad(las, 4, pad = "0")

SurveyData$MRN <- as.numeric(paste(fir,las, sep = ""))

SurveyData$MRN[SurveyData$MRN == 201059530] <- 20155330

SurveyData$MRN[SurveyData$MRN == 201445390] <- 20145390

SurveyData$MRN[SurveyData$MRN == 20071040] <- 20070104

SurveyData$MRN[SurveyData$MRN == 20800619] <- 20080619

gov_total <- gov_total %>% 
  dplyr::select(MRN, Governorate)

SurveyData <- left_join(SurveyData ,gov_total)

#ff <- anti_join(ddd, gov_total )


SurveyData$governorate[SurveyData$Governorate %in% c("Behera", "Dakahlya", "Damietta", "Gharbya", "Ismaeilya", "Kafr EL-Sheikh", "Menoufya", "Qalyoubia", "Sharkya")] <- "Lower Egypt"

SurveyData$governorate[SurveyData$Governorate %in% c("Assyout", "Aswan", "Beny Swef", "Luxor", "Menia", "Qena", "Sohag", "Giza", "Fayoum")] <- "Upper Egypt"

SurveyData$governorate[SurveyData$Governorate %in% c("Cairo", "Alexandria" , "Port-Saeid", "Suez")]  <- "Urban Governorates"

SurveyData$governorate[SurveyData$Governorate %in% c("South Sinai", "Red Sea", "North Sinai", "Matrouh", "New Valley"  )]  <- "Frontier Governorates"


SurveyData$governorate[SurveyData$Governorate %in% c("Outside Egypt")]  <- "Outside Egypt"

#table(SurveyData$governorate)

SurveyData$family_income_total = SurveyData$Father_month_salary + SurveyData$Mother_month_salary + SurveyData$other_income_val

SurveyData <- filter(SurveyData, author != "other")

#---------------- recode variables => salary, education ---------------------------
## salary
SurveyData$income_category =ifelse(
  SurveyData$family_income_total <= 2000, " 1_less than 2000",
  ifelse(SurveyData$family_income_total > 2000 & SurveyData$family_income_total <= 6000  ,"2_from 2000 to 6000",
         ifelse(SurveyData$family_income_total > 6000 & SurveyData$family_income_total <= 12000  ,"3_from 6000 to 12000",
                ifelse(SurveyData$family_income_total > 12000 & SurveyData$family_income_total <= 25000  ,"4_from 12000 to 25000",
                       "5_more than 25000"
                )
         )
  )
)


##education

SurveyData$father_edu_recoded =ifelse(
  SurveyData$father_edu == "Illiterate", " 1_Illiterate",
  ifelse(SurveyData$father_edu == "read n write","2_read n write",
         ifelse(SurveyData$father_edu == "primary","3_primary",
                ifelse(SurveyData$father_edu =="prep","4_prep",
                       ifelse(SurveyData$father_edu =="secondary +","4_secondary +",
                              ifelse(SurveyData$father_edu =="University Degree","5_University Degree",
                                     
                                     "Master & PHD"
                              )
                       )
                )
         )
  )
)



library(psych)
library(GPArotation)

# get the mean for all questions (before questions removal)

questions <- SurveyData[, 2:62]

# ---------------------------  combining categorical variables ---------------------------------

# I will combine levels of income category


SurveyData$income_category_comb <- SurveyData$income_category
SurveyData$income_category_comb[SurveyData$income_category_comb == "5_more than 25000"] <- "4_from 12000 to 25000"
SurveyData$income_category_comb[SurveyData$income_category_comb == "4_from 12000 to 25000"] <- "4_more than 12000"


# Now we are having only 4 categories, the smallest contains 22 subjects 

# I will combine father education levels 

SurveyData$father_edu_comb <- SurveyData$father_edu_recoded
SurveyData$father_edu_comb[SurveyData$father_edu_comb == "Master & PHD"] <- "5_University Degree"


SurveyData$father_edu_combined <- as.factor(SurveyData$father_edu_comb)

levels(SurveyData$father_edu_combined) <- c("Illiterate", "R&W_prim", "R&W_prim", "prep/sec", "prep/sec", "University Degree & post-grad")


total_mean <- data.frame(cbind(SurveyData[, 1], rowMeans(questions), SurveyData$father_edu_combined, SurveyData$income_category_comb, SurveyData$sex))
colnames(total_mean) <- c("ID", "Mean_score", "father_education", "income_category", "sex")

SurveyData$mo_edu <- SurveyData$Mother_education

SurveyData$mo_edu <- car::recode(SurveyData$mo_edu, "'Illiterate' = 1; c('read n write', 'primary') = 2; 
  c('prep', 'secondary +') = 3; 'University Degree' = 4")


SurveyData$fa_edu <- car::recode(SurveyData$father_edu_combined, "'Illiterate' = 1; 'R&W_prim' = 2; 'prep/sec' = 3; 'University Degree & post-grad' = 4")

SurveyData$parent_education <- SurveyData$fa_edu

SurveyData$parent_education[SurveyData$author == "mother"] <- SurveyData$mo_edu[SurveyData$author == "mother"]

levels(SurveyData$parent_education) <- c("Illiterate", "R&W-PRIM", "PREP-SEC", "University")

```

Apart from few missing data in demographics (table 1), nothing was missing regarding the survey answers to all questions. We calculated total family income by summing up father monthly salary, mother monthly salary as well as other incomes gained from other resources. We tried to categorize income and father education into meaningful categories, with an attempt to avoid small number of participants in subgroups so as to have valid statistical comparisons.  

```{r}

SurveyData$parent <- SurveyData$author

levels(SurveyData$parent) <- c("father", "mother", "mother")

SurveyData$Number_of_family_members <-  cut(SurveyData$family_members, c(-1, 4, 6, 100), c("<=4", "5-6", ">=7"))
  
table1(~ sex +  governorate + parent + Number_of_family_members +  governorate + parent_education + income_category_comb , data=SurveyData,  topclass="Rtable1-zebra")

```

# proper questions 

```{r}

ques53 <- questions %>% 
  dplyr::select(-c(7, 12, 23, 30, 9, 47, 16, 49))

ques52 <- questions %>% 
  dplyr::select(-c(7, 12, 23, 30, 9, 47, 16, 49, 51))

ques51 <- questions %>% 
  dplyr::select(-c(7, 12, 23, 30, 9, 47, 16, 49, 51, 11))

ques49 <- questions %>% 
  dplyr::select(-c(7, 12, 23, 30, 9, 47, 16, 49, 51, 11, 25, 40))

ques48 <- questions %>% 
  dplyr::select(-c(7, 12, 23, 30, 9, 47, 16, 49, 51, 11, 25, 40, 46)) # I will keep 46 

ques49rep <- questions %>% 
  dplyr::select(-c(7, 12, 45, 30, 9, 47, 16, 49, 51, 11, 25, 40))   # replaced 23 with 45

ques48_2 <- questions %>% 
  dplyr::select(-c(7, 12, 23, 30, 9, 47, 16, 49, 51, 11, 25, 40, 45)) # I will keep 46, remove both 45 and 23 

ques48_3 <- questions %>% 
  dplyr::select(-c(7, 12, 23, 30, 9, 47, 16, 49, 51, 11, 25, 40, 41))# removed 41 

ques48_4 <- questions %>% 
  dplyr::select(-c(7, 12, 23, 30, 9, 47, 16, 49, 51, 11, 25, 40, 26))# removed 41 

ques48_5 <- questions %>% 
  dplyr::select(-c(7, 12, 23, 30, 9, 47, 16, 49, 51, 11, 25, 40, 59))# removed 41 


# ques45 <- questions %>% 
#   dplyr::select(-c(6,9,12,18,53,3,47, 55,61,44,50, 16,17,49,51, 40))
# 
# ques44 <- questions %>% 
#   dplyr::select(-c(6,9,10, 11,12,18,53,3,47, 55,44,50, 16,17,49,51, 40))

```

# MIRT

### modern psychometric with R book

##### Dimentionality assessment

```{r}
library("MPsychoR")
library("mirt")
library("Gifi")
dim_ques <- princals(ques53)
plot(dim_ques, main = "Irrational thoughts loadings")

```

unidimensionality is violated

### check number of factors 

```{r}
no_fact <- nfactors(ques53, n = 4, cor = "poly", rotate = "oblimin")
# vss = 2
# MAP = 4
# emperical BIC = 9
# ss adjusted BIC = 20


```

```{r}
resvss <- vss(ques53, n.obs = nrow(ques53), plot = FALSE, rotate = "oblimin")

# VSS complexity 1 achieves a maximimum of 0.62  with  1  factors
# VSS complexity 2 achieves a maximimum of 0.72  with  2  factors
# 
# The Velicer MAP achieves a minimum of 0  with  2  factors 
# BIC achieves a minimum of  -5626.4  with  3  factors
# Sample Size adjusted BIC achieves a minimum of  -1942.75  with  5  factors
```



```{r}
#trial one dimension

faques53_one <- fa(ques53, 1, cor = "poly")
summary(faques53_one)
# very very low TLI 

faques53_two <- fa(ques53, 2, cor = "poly")
summary(faques53_two)


# improved estimates 
# TLI : 0.625

faques53_three <- fa(ques53, 3, cor = "poly")
summary(faques53_three)

# TLI : 0.646

faques53_four <- fa(ques53, 4, cor = "poly")
summary(faques53_four)

# TLI : 0.664

```


### scree plot

```{R}
Rdep <- polychoric(ques53)$rho
evals <- eigen(Rdep)$values
scree(Rdep, factors = FALSE)

```


```{r}

plot(dim_ques, "screeplot")
```

2 factors or 3  or 4 or 5 



```{r}

parallel2 <- fa.parallel(ques53,nfactors = 2, cor = "poly" )

parallel4 <- fa.parallel(ques53,nfactors = 4, cor = "poly" )

parallel6 <- fa.parallel(ques53,nfactors = 6, cor = "poly" )

parallel8 <- fa.parallel(ques53,nfactors = 8, cor = "poly" )

# documentation :
# A sad observation about parallel analysis is that it is sensitive to sample size. That is, for large data sets, the eigen values of random data are very close to 1. This will lead to different estimates of the number of factors as a function of sample size. Consider factor structure of the bfi data set (the first 25 items are meant to represent a five factor model). For samples of 200 or less, parallel analysis suggests 5 factors, but for 1000 or more, six factors and components are indicated. This is not due to an instability of the eigen values of the real data, but rather the closer approximation to 1 of the random data as n increases.
# 
# Although with nfactors=1, 6 factors are suggested, when specifying nfactors =5, parallel analysis of the bfi suggests 12 factors should be extracted

```
18 factors and 6 components. 




### IFA method 

```{r}
# mirt fits a maximum likelihood (or maximum a posteriori) factor analysis model to any mixture of dichotomous and polytomous data under the item response theory paradigm. Thus I don't have to specify "poly"
mod_graded <- mirt(ques53, 1, itemtype = "graded")
mod_graded2 <- mirt(ques53, 2, itemtype = "graded", TOL = 0.001)

anova(mod_graded, mod_graded2, verbose = T)

mod_graded3 <- mirt(ques53, 3, itemtype = "graded", TOL = 0.001)
anova(mod_graded3, mod_graded2, verbose = T)


mod_graded4 <- mirt(ques53, 4, itemtype = "graded", TOL = 0.001)
anova(mod_graded4, mod_graded3, verbose = T)
# slight decrease in AIC BIC 


anova(mod_graded4, mod_graded2, verbose = T)

anova(mod_graded3, mod_graded4, verbose = T)

# 4 models doesn't make sense, slight improvement


# I would go for 3 factors  


```
AIC BIC and log likelihood changed dramatically when using 2 factors instead of 1 

this is also true when comparing two and three factors.  I will use 3 factors but I will still confirm with Dr. Essam. 
```{r}
# removing one by one 

### removing question 51 

# mod_52_graded3 <- mirt(ques52, 3, itemtype = "graded", TOL = 0.001)
# 
# anova(mod_graded3, mod_52_graded3, verbose = T) # p value is not significant, however, almost everything has imrpoved to a relatively high extent
# 
# stat_52_graded3 <- M2(mod_52_graded3) # didn't use type C2 since I have too many dfs 
# 
# ifit_ques52_graded <- mirt::itemfit(mod_52_graded3)
# 
# ifit_ques52_graded[ifit_ques52_graded[, 5] < 0.05, ] ## misfitting items
# 

### removing question 11

mod_51_graded3 <- mirt(ques51, 3, itemtype = "graded", TOL = 0.001)

anova(mod_51_graded3, mod_graded3, verbose = T) # p value is not significant, however, almost everything has improved to a relatively high extent
# after removal of questions 51 and 11, improvement is even bigger. 

### removing 25, 40

mod_49_graded3 <- mirt(ques49, 3, itemtype = "graded", TOL = 0.001)

anova(mod_51_graded3, mod_49_graded3, verbose = T) # p value is not significant, however, almost everything has imrpoved 

anova(mod_49_graded3, mod_graded3, verbose = T) # p value is not significant, however, almost everything has imrpoved 

stat_49_graded3 <- M2(mod_49_graded3) # didn't use type C2 since I have too many dfs 
 
ifit_ques49_graded <- mirt::itemfit(mod_49_graded3)

ifit_ques49_graded[ifit_ques49_graded[, 5] < 0.05, ] ## misfitting items

## removing 46
# 
# mod_48_graded3 <- mirt(ques48, 3, itemtype = "graded", TOL = 0.001)
# 
# anova(mod_48_graded3, mod_49_graded3, verbose = T) # p value is not significant, however, almost everything has imrpoved 
# 
# anova(mod_48_graded3, mod_graded3, verbose = T) # p value is not significant, however, almost everything has imrpoved 
# 
# stat_48_graded3 <- M2(mod_48_graded3) # didn't use type C2 since I have too many dfs 
#  
# ifit_ques48_graded <- mirt::itemfit(mod_48_graded3)
# 
# ifit_ques48_graded[ifit_ques48_graded[, 5] < 0.05, ] ## misfitting items
# 
# 
# 
# mod_48_2_graded3 <- mirt(ques48_2, 3, itemtype = "graded", TOL = 0.001)
# 
# anova(mod_48_2_graded3, mod_49_graded3, verbose = T) # p value is not significant, however, almost everything has imrpoved 
# 
# stat_48_2_graded3 <- M2(mod_48_2_graded3) # didn't use type C2 since I have too many dfs 
# 
# # remove 41
# 
# mod_48_3_graded3 <- mirt(ques48_3, 3, itemtype = "graded", TOL = 0.001)
# 
# anova(mod_48_3_graded3, mod_49_graded3, verbose = T) # p value is not significant, however, almost everything has imrpoved 
# 
# stat_48_3_graded3 <- M2(mod_48_3_graded3) # didn't use type C2 since I have too many dfs 
# 
# # End of trial. I will then examine questions based on p-value
# 
# # removing question 26
# 
# mod_48_4_graded3 <- mirt(ques48_4, 3, itemtype = "graded", TOL = 0.001)
# 
# anova(mod_48_4_graded3, mod_49_graded3, verbose = T) # p value is not significant, however, almost everything has imrpoved 
# 
# stat_48_4_graded3 <- M2(mod_48_4_graded3) # didn't use type C2 since I have too many dfs 
# 
# 
# 
# 
# # removing question 59
# 
# mod_48_5_graded3 <- mirt(ques48_5, 3, itemtype = "graded", TOL = 0.001)
# 
# anova(mod_48_5_graded3, mod_49_graded3, verbose = T) # p value is not significant, however, almost everything has imrpoved 
# 
# stat_48_5_graded3 <- M2(mod_48_5_graded3) # didn't use type C2 since I have too many dfs 
# 
# 

```

```{r}
# foo <- summary(mod_51_graded3, rotate = "oblimin")
# 
# trial_all <- data.frame(foo$rotF) %>%
#   rownames_to_column('gene') %>%
#   filter_if(is.numeric, all_vars(abs(.) < 0.3)) %>%
#   column_to_rownames('gene')   # get only questions that have correlation less than 0.3 with all other variables. 
# 
# foo_v <- summary(mod_51_graded3, rotate = "varimax")
# 
# trial_all_v <- data.frame(foo_v$rotF) %>%
#   rownames_to_column('gene') %>%
#   filter_if(is.numeric, all_vars(abs(.) < 0.3)) %>%
#   column_to_rownames('gene')   # get only questions that have correlation less than 0.3 with all other variables. 


foo <- summary(mod_49_graded3, rotate = "oblimin")

trial_all <- data.frame(foo$rotF) %>%
  rownames_to_column('gene') %>%
  filter_if(is.numeric, all_vars(abs(.) < 0.3)) %>%
  column_to_rownames('gene')   # get only questions that have correlation less than 0.3 with all other variables. 

foo_v <- summary(mod_49_graded3, rotate = "varimax")

trial_all_v <- data.frame(foo_v$rotF) %>%
  rownames_to_column('gene') %>%
  filter_if(is.numeric, all_vars(abs(.) < 0.3)) %>%
  column_to_rownames('gene')   # get only questions that have correlation less than 0.3 with all other variables. 



```




##### I have chosen this method since Dave suggested using M2 

I have chosen graded response model since it appeared from Dr Essam talk that maybe some people are afraid to answer, thus they may choose "sometime crosses my mind" while in fact they mean this thing. So I thought graded response model would be better at describing the data. 

```{r}

#mod_gpcm <- mirt(ques53, 2, itemtype = "gpcm")
# mod_pc <- mirt(ques53, 2, itemtype = "PC3PL") Partially compensatory models can only be estimated within a confirmatory model
#mod_ggum <- mirt(ques53, 2, itemtype = "ggum")  not used. check MIRT word document 
#mod_seq <- mirt(ques53, 2, itemtype = "sequential", TOL = 0.001) # not used, check MIRT word document
#mod_spline <- mirt(ques53, 2, itemtype = "spline") # no restrictions on the model regarding maths, over sophisticated 

# stat_graded2 <- M2(mod_graded2) # didn't use type C2 since I have too many dfs 
# stat_graded3 <- M2(mod_graded3) # didn't use type C2 since I have too many dfs 

stat_51_graded3 <- M2(mod_51_graded3) # didn't use type C2 since I have too many dfs 


#stat_gpcm <- M2(mod_gpcm)

```
statistics reveal here that 3 factors is better than 2 factors 

This even better after removing questions 51 and 11

as per the modern psychometrics book 
For the CFI, we can use the same 0.95 fit
cutoff as in CFA/SEM. For the RMSEA, the CFA/SEM cutoff was 0.05 for a good
fitting model. In IRT, it is suggested to use 0.05/k (with k being the number of
categories per item) as fit cutoff.


https://groups.google.com/forum/#!topic/mirt-package/gy63tCz2W98   ( same as SEM, 0.9 for CFI and 0.05 for RMSEA)

Do you have a large sample? If so could be that small differences in expected vs observed response freqs are causing significant p values even though magnitude of misfit is not that big per empirical item plots (and S-X2 RMSEA) https://groups.google.com/forum/#!searchin/mirt-package/M2|sort:date/mirt-package/ZWFimGtUEsI/MXx6pXoaBgAJ 

 
### item fit 

##### graded 

```{r}

ifit_ques51_graded <- mirt::itemfit(mod_51_graded3)

ifit_ques51_graded[ifit_ques51_graded[, 5] < 0.05, ] ## misfitting items
# 
# ifit_ques53_graded <- mirt::itemfit(mod_graded3)
# 
# ifit_ques53_graded[ifit_ques53_graded[, 5] < 0.05, ] ## misfitting items

```
A single questions is slightly missfitting, and I would like to keep it. 


```{r}
##### gpcm 

# ifit_ques53_gpcm <- mirt::itemfit(mod_gpcm)
# 
# ifit_ques53_gpcm[ifit_ques53[, 5] < 0.05, ] ## misfitting items

```

Here is it recommended by factor loadings that we should remove 5 questions. (25,27,40,41,46)

On examining the questions, it appears that some questions should in fact be removed as 25 and 40.. 

On chaning the rotation to varimax, three questions are consistent (25,27,40)

I will remove 25 and 40, since there phrasing is strange and I will recheck.

```{r}

```


```{R}
itemplot(mod_51_graded3, 3, main = "ICS addit3",
rot = list(xaxis = -70, yaxis = 50, zaxis = 10))

```

```{r}
coef(mod_51_graded3)

```
### multidimentional item location 

```{r}
head(MDIFF(mod_51_graded3))

```

### person paramegters 

```{r}

head(fscores(mod_51_graded3))

```

### residuals

```{r}

res <- residuals(mod_51_graded3, type = "LDG2", digits = 4, df.p = T)

```



# exploratory multigroup MIRT

### for parent 

```{r}

class2 <- SurveyData$parent
levels(class2)
modMG <- multipleGroup(ques53, model = 2, group = class2,
SE = TRUE, itemtype = "graded", TOL = 0.01)  # model =2 means two dimentional model
# https://groups.google.com/forum/#!searchin/mirt-package/multiplegroup$20exploratory|sort:date/mirt-package/j-ZyjuXPNcY/zoSTk-BEBgAJ 

# which p-value to use ?  Ans: 0.05, but u can use 0.01 
# https://groups.google.com/forum/#!searchin/mirt-package/multiplegroup$20exploratory|sort:date/mirt-package/CXWkXsKCBvo/WAivQ5ABBQAJ

astiDIF <- DIF(modMG, c('a1', 'd'), Wald = TRUE,
p.adjust = 'fdr')
round(astiDIF$adj_pvals[astiDIF$adj_pvals < 0.05], 4)
```

no DIF items 

### parent education 

```{r}

class3 <- SurveyData$parent_education
levels(class3) <- c("ILL", "rwp", "psec", "uni")

modMG_edu <- multipleGroup(ques53, model = 2, group = class3, itemtype = "graded", TOL = 0.01, SE = T)  

DIF_edu <- DIF(modMG_edu, c('a1', 'd'), Wald = T,
p.adjust = 'fdr', simplify = T, verbose = T)
round(DIF_edu$adj_pvals[astiDIF$adj_pvals < 0.05], 4)



```

### income category 

```{r}

class4 <- factor(SurveyData$income_category)
modMG_inc <- multipleGroup(ques53,  model = 2, group = class4, itemtype = "graded", TOL = 0.01, SE= T)

DIF_inc <- DIF(modMG_inc, c('a1', 'd'), Wald = TRUE,
p.adjust = 'fdr')
round(DIF_inc$adj_pvals[astiDIF$adj_pvals < 0.05], 4)


```


### Number of children 

```{r}

class5 <- factor(SurveyData$Number_of_family_members)
modMG_mem <- multipleGroup(ques53,  model = 2, group = class5, itemtype = "graded", TOL = 0.01, SE= T)

DIF_mem <- DIF(modMG_mem, c('a1', 'd'), Wald = TRUE,
p.adjust = 'fdr')
round(DIF_mem$adj_pvals[astiDIF$adj_pvals < 0.05], 4)

```



# TRY after removal 2 questions

```{R}
ques43_irt <- ques53 %>% 
  dplyr::select(- c(40,44))

mod_51_graded3_43 <- mirt(ques43_irt, 2, itemtype = "graded")

stat_graded_43 <- M2(mod_51_graded3_43) # didn't use type C2 since I have too many dfs 

```

# Try after removal 20 questions 

```{r}
ques19_irt <- ques53 %>% 
  dplyr::select(- c(20:61))

mod_51_graded3_19 <- mirt(ques19_irt, 1, itemtype = "graded")

stat_graded_19 <- M2(mod_51_graded3_19) # didn't use type C2 since I have too many dfs 

```


```{r}

plot(mod_graded3, type = "info", main = "Item Information")

# This plot (not shown here) tells us in which trait area our entire scale is
# informative and thus able to assess a personâ€™s location on the conservatism trait
# with good precision

```


### Limitations

We have used only three levels of likert scale questions. Results could have been more powerful if we used at least five levels. In addition, using an odd number of levels may drive the responder to choose the middle response. On the contrary, using an even number of levels could have avoided this. 





# Supplementary figures 

### likert plot for demand factor

```{r fig.height= 15, fig.width= 15}
#https://cran.r-project.org/web/packages/HH/HH.pdf

lkrt_plot <- function(factor, name){
  
  fct <- data.frame(cbind(SurveyData[1], factor ))
  
  fct <- gather(fct, "question", "response", -ID)
  
  g <- fct %>% group_by(question, response) %>% 
    summarise(n = n()) %>% 
    arrange(response)
  
  g <- reshape2::melt(g, id.vars=c("question", "response"))
  names(g)[3] <- "Agreement"
  
  g$response <- as.factor(g$response)
  
  levels(g$response) <- c("never", "sometimes", "common")
  
  return(likert(question ~ response , value="value", data=g,
         main = name,
         as.percent = T,
         ylab=NULL,
         scales=list(y=list(relation="free")), layout=c(1,2)))
  
}


```


We checked whether irrational believes scores vary according to parent or child factors. We applied multivariate analysis of variance instead of univariate ANOVA or t-test to avoid family-wise error rate(alpha inflation). In addition, this enables us to preserve power since scale scores are correlated. 



Parametric tests as MANOVA should be used with caution with ordinal data like the likert scale we are dealing with, as assumptions maybe violated. However, we are using the average score for the questions. This renders the questions in a continuous rather than a categorical form. The same methodology has been previously applied in a similar article written by [@bonnerDevelopmentValidationParent2006]. Moreover, with the large sample size we have, normality is never a problem. [click to view reference](https://www.st-andrews.ac.uk/media/capod/students/mathssupport/OrdinalexampleR.pdf). Box's M test was used to test the assumption of homogeneity of variances and covariances and the assumption was not violated.  

```{r}

# MANOVA for gender

# Assumptions of MANOVA

# MANOVA can be used in certain conditions:
# 
# The dependent variables should be normally distribute within groups. The R function mshapiro.test( )[in the mvnormtest package] can be used to perform the Shapiro-Wilk test for multivariate normality. This is useful in the case of MANOVA, which assumes multivariate normality.
# 
# Homogeneity of variances across the range of predictors.
# 
# Linearity between all pairs of dependent variables, all pairs of covariates, and all dependent variable-covariate pairs in each cell

library(RVAideMemoire)

#mshapiro.test(cor_data)

# here I tried to test for multivariate normality, however, due to the large sample size, p value of shapiro wilk test is definetely significant. Thus, we will shift to the package MVN: An R Package for Assessing Multivariate Normality  

library(MVN)

# result <- mvn(data = cor_data, mvnTest = "mardia")
# result$multivariateNormality
# 
# 
# result <- mvn(data = cor_data, mvnTest = "hz")
# result$multivariateNormality
# 
# 
# result <- mvn(data = cor_data, mvnTest = "royston")
# result$multivariateNormality
# # this depends on shapiro, so shouldn't be used with a large sample size
# 
# 
# result <- mvn(data = cor_data, mvnTest = "dh")
# result$multivariateNormality
# 
# 
# result <- mvn(data = cor_data, mvnTest = "energy")
# result$multivariateNormality

# here it appears that for all tests, the normality assumption is violated, however, this is not a problem because we have a large sample size. 


# mnva_qplot <- function(data){
#   # create univariate Q-Q plots
# mvn(data = data, mvnTest = "royston", univariatePlot = "qqplot")
# }
# 
# dsc_table <- mnva_qplot(cor_data)
# 
# dsc_table <- dsc_table$Descriptives[1:8]
# 
# rmdtable(dsc_table)

# mnva_hist <- function(data){   # create univariate histograms
# 
#   mvn(data = cor_data, mvnTest = "royston", univariatePlot = "histogram")
# }
# 
# mnva_hist(cor_data)


# From the plots, it appears that demand factor is the most problematic. 
# However, again this is not a problem

# https://cran.r-project.org/web/packages/MVN/vignettes/MVN.pdf  the perfect package


# Now it is time to test for homogenity of variance assumption 

 # create a data set for three mult columns and sex 

test_data <- cbind(cor_data, SurveyData$sex)

colnames(test_data)[4] <- "sex"

library(micompr)

#assumptions_manova(cor_data, test_data$sex)

# Box's M test is a multivariate statistical test used to check the equality of multiple variance-covariance matrices.[1] The test is commonly used to test the assumption of homogeneity of variances and covariances in MANOVA and linear discriminant analysis. It is named after George E. P. Box.
# 
# Box's M test is susceptible to errors if the data does not meet model assumptions or if the sample size is too large or small.[2] Box's M test is especially prone to error if the data does not meet the assumption of multivariate normality


# here it appears that multivariate homogenity of variance assumption is not violated


################

# datacamp made a quick reference for MANOVA with assumptions 

# I will move with this guide step by step 

# https://www.statmethods.net/stats/anova.html 

mnv_tbl <- function(col){
  
  fit_sex <- tidy(manova(as.matrix(cor_data) ~ col), test = "Hotelling-Lawley")

fit_sex <- fit_sex[1, c(1,2,3,4,7)]

colnames(fit_sex) <- c("Factor", "df", "Hotelling-Lawley trace", "F", "p.value")

return(fit_sex)  
}

mnv_sex <- mnv_tbl(SurveyData$sex)

# fit_sex <- tidy(manova(as.matrix(cor_data) ~ SurveyData$sex, test = "Hotelling-Lawley"), test = "Hotelling-Lawley")
# 
# fit_sex <- fit_sex[1, c(1,2,3,4,7)]
# 
# colnames(fit_sex) <- c("Factor", "df", "Hotelling-Lawley trace", "F", "p.value")


# for gender, we have 1 degree of freedom, so all tests should be identical. 
# however, here wilks is different and I don't know why 

# summary(fit, test="Pillai")
# 
# summary(fit, test="Wilks")
# 
# summary(fit, test="Hotelling-Lawley")
# 
# summary(fit, test="Roy")


# When the hypothesis degrees of freedom, h, is one, all four test statistics will lead to identical results. When h>1,
# the four statistics will usually lead to the same result. When they do not, the following guidelines from
# Tabachnick (1989) may be of some help.
# Wilks' Lambda, Lawley's trace, and Roy's largest root are often more powerful than Pillai's trace if h>1 and one
# dimension accounts for most of the separation among groups. Pillai's trace is more robust to departures from
# assumptions than the other three.
# Tabachnick (1989) provides the following checklist for conducting a MANOVA. We suggest that you consider
# these issues and guidelines carefully.

# significant result, and it is the same value for all tests

#summary.aov(fit, test="Wilks")  # univariate anova

# checking assumptions

# The aq.plot() function in the mvoutlier package allows you to identfy multivariate outliers by plotting the ordered squared robust Mahalanobis distances of the observations against the empirical distribution function of the MD2i. Input consists of a matrix or data frame. The function produces 4 graphs and returns a boolean vector identifying the outliers.
library(mvoutlier)

# outliers <- aq.plot(cor_data)
# outliers # show list of outliers

# testing normality 

# mshapiro.test(as.matrix(cor_data))
# 
# # Graphical Assessment of Multivariate Normality
# x <- as.matrix(cor_data) # n x p numeric matrix
# center <- colMeans(x) # centroid
# n <- nrow(x); p <- ncol(x); cov <- cov(x); 
# d <- mahalanobis(x,center,cov) # distances 
# qqplot(qchisq(ppoints(n),df=p),d,
#   main="QQ Plot Assessing Multivariate Normality",
#   ylab="Mahalanobis D2")
#abline(a=0,b=1)

```



```{r}

# sex_manova <- rmdtable(tidy(HotellingsT2(as.matrix(cor_data) ~ SurveyData$sex)))

# SPSS will give Hotelling's Trace, and it has to convert to Hotelling's T^2 as follows:
#   Multiplying Hotelling's Trace by (N - L), where N is the sample size across all groups and L is the number of groups, gives a generalized version of Hotelling's T^2.

# https://www.researchgate.net/post/How_can_I_do_Hotellings_T-square 

#Parent score differ according to child's gender. 

```



```{r}

### MANOVA father education 

# SurveyData$mo_edu <- SurveyData$Mother_education
# 
# SurveyData$mo_edu <- car::recode(SurveyData$mo_edu, "'Illiterate' = 1; c('read n write', 'primary') = 2; 
#   c('prep', 'secondary +') = 3; 'University Degree' = 4")
# 
# 
# SurveyData$fa_edu <- car::recode(SurveyData$father_edu_combined, "'Illiterate' = 1; 'R&W_prim' = 2; 'prep/sec' = 3; 'University Degree & post-grad' = 4")
# 
# SurveyData$parent_education <- SurveyData$fa_edu
# 
# SurveyData$parent_education[SurveyData$author == "mother"] <- SurveyData$mo_edu[SurveyData$author == "mother"]
# 
# levels(SurveyData$parent_education) <- c("Illiterate", "R&W-PRIM", "PREP-SEC", "University")
# 
edu_data <- cbind(cor_data, SurveyData$parent_education)

colnames(edu_data)[4] <- "edu"

edu_data <- edu_data[complete.cases(edu_data),]

#assumptions_manova(edu_data[1:3], as.factor(edu_data$edu))

# here the homogenity of variance assumption is not violated
#although the test is sensitive to large numbers, meaning that it may cause the assumption to be violated even when tiny departures from hokmogenity are present. 
#however, here it is not violated.

fit_edu <- manova(as.matrix(cor_data) ~ SurveyData$parent_education)

m <- list(summary(fit_edu, test="Pillai"),
summary(fit_edu, test="Wilks"),
summary(fit_edu, test="Hotelling-Lawley"),
summary(fit_edu, test="Roy"),
summary.aov(fit_edu, test="Wilks")  # univariate anova
)

mnv_edu <- mnv_tbl(SurveyData$parent_education)

```

```{r}

### MANOVA for income

inc_data <- cbind(cor_data, SurveyData$income_category_comb)

colnames(inc_data)[4] <- "inc"
inc_data <- inc_data[complete.cases(inc_data),]

#assumptions_manova(inc_data[1:3], as.factor(inc_data$inc))

# homogenity of variance assumption is not violated

mnv_income <- mnv_tbl(SurveyData$income_category_comb)

fit <- manova(as.matrix(inc_data[1:3]) ~ inc_data$inc)
#tidy(fit, test = "Hotelling-Lawley")

m <- list(summary(fit, test="Pillai"),
summary(fit, test="Wilks"),
summary(fit, test="Hotelling-Lawley"),
summary(fit, test="Roy"),
summary.aov(fit, test="Wilks")  # univariate anova
)

 # it is highly significant for all

```


```{r}

# ANOVA parent gender

parent_data <- cbind(cor_data, SurveyData$author)

colnames(parent_data)[4] <- "parent"

parent_data <- parent_data[complete.cases(parent_data),]

#assumptions_manova(parent_data[1:3], as.factor(parent_data$parent))

#assumption not working and I don't know why

#assumptions_manova(parent_data[1:3], parent_data$parent)

# homogenity of variance assumption is not violated

mnv_parent <- mnv_tbl(SurveyData$parent_education)
# results of the function are wrong 

fit_parent <- manova(as.matrix(parent_data[1:3]) ~ parent_data$parent)

fit_parent <- tidy(fit_parent, test = "Hotelling-Lawley")

m <- list(summary(fit, test="Pillai"),
summary(fit, test="Wilks"),
summary(fit, test="Hotelling-Lawley"),
summary(fit, test="Roy"),
summary.aov(fit, test="Wilks")  # univariate anova
)

fit_parent <- fit_parent[1, c(1,2,3,4,7)]

colnames(fit_parent) <- c("Factor", "df", "Hotelling-Lawley trace", "F", "p.value")

mnv_tbls <- rbind(mnv_sex, fit_parent, mnv_edu, mnv_income )
 # it is highly significant for all

mnv_tbls$Factor <- c("child gender", "parent gender", "parent education", "family income")

mnv_tbls <- rmdtable(mnv_tbls)

mnv_tbls <- color(mnv_tbls, i = ~ p.value < 0.05, j = ~ p.value , color = "red")
mnv_tbls
```
Table MANOVA results for parent and children factors

The scale scores varied significantly. with child gender, family income and parent education but not with parent gender. For child gender and parent gender, the degrees of freedom were 1. For all results, the four tests, Pillai, Wilks, Hotelling-Lawley and Roy provided quite similar results and here we report Hotelling-Lawley trace results. 


### Post-hoc MANOVA family income


```{r}

levels(inc_data$inc) <- c("<2", "2-6", "6-12", ">12")

inc_post <- mergeFactors(response = inc_data[,1:3],
                                   factor = factor(inc_data$inc),
                                   method = "adaptive") 

plot(inc_post, panel = "GIC", nodesSpacing = "effects", colorCluster = TRUE)

```

The higher the family income, the less common irrational believes are. 

### Domains' Averages according to income group 


```{r}

inc_mean <- inc_data %>% 
  group_by(inc) %>%
  summarise_all(mean)

rmdtable(inc_mean)
```


### Significance of groups splitting 


```{r}

inc_post_h <- mergingHistory(inc_post, showStats = TRUE) 
inc_post_h <- rmdtable(inc_post_h)

inc_post_h <- color(inc_post_h, i = ~ pvalVsPrevious < 0.05, j = ~ pvalVsPrevious , color = "red")

inc_post_h

```

Each row of the above frame describes one step of the merging algorithm. First two columns specify which groups were merged in the iteration. Last two columns are p-values for the Likelihood Ratio Test - against the full model (pvalVsFull) and against the previous one (pvalVsPrevious). Only the last step is significant, splitting the income categories into less than 6000 and more than 6000. 


### Post hoc MANOVA parent education level


```{r fig.height= 12, fig.width= 12}

library(factorMerger)

# post hoc MANOVA education

edu_post <- mergeFactors(response = edu_data[,1:3],
                                   factor = factor(edu_data$edu),
                                   method = "adaptive") 

plot(edu_post, panel = "GIC", nodesSpacing = "effects", colorCluster = TRUE)

```

Parents with a university degree or higher experience less commonly irrational believes than others with lower educational level. 


### Domains' Averages according to income group 


```{r}

edu_mean <- edu_data %>% 
  group_by(edu) %>%
  summarise_all(mean)

rmdtable(edu_mean)

```

### Significance of groups splitting 


```{r}

edu_post_h <- mergingHistory(edu_post, showStats = TRUE) 
edu_post_h <- rmdtable(edu_post_h)

edu_post_h <- color(edu_post_h, i = ~ pvalVsPrevious < 0.05, j = ~ pvalVsPrevious , color = "red")

edu_post_h
# data_edu <- SurveyData

# data_edu <- filter(data_edu, !is.na(fa_edu), !is.na(mo_edu))
# 
# data_edu$max_edu <- pmin(data_edu$fa_edu, data_edu$mo_edu)
# 
# n <- data.frame(cbind(data_edu$fa_edu, data_edu$mo_edu)) # I am using this step, because pmax is producing wrong results in the large dataframe
# 
# n$max <- pmax(n$X1, n$X2)
# 
# data_edu$max_edu <- n$max
# # table(n$max)
# table(data_edu$max_edu)


```

Splitting university from all other educational levels yielded significant results. 



```{r}

# We found that score differs significantly for the total questions. However, bear is the only factor where sex is found to be significantly affecting the score. 


# demand_p <- var.test(demand ~ sex, data = dat.clean)
# 
# demand_p <- demand_p$p.value
# 
# bear_p <- var.test(bear ~ sex, data = dat.clean)
# bear_p <- bear_p$p.value
# 
# accuse_p <- var.test(accuse ~ sex, data = dat.clean)
# accuse_p <- accuse_p$p.value
# 
# cbind(demand_p, bear_p, accuse_p)
# # combining box plots 

# # ggboxplot(dat.clean, x = "father_edu_combined", y = "cognitive_dim", 
# #           color = "income_category_comb",
#           ylab = "Mean score", xlab = "Income category")

```


### detect redundant questions 

```{r}
all_q <- round(all_qu$rho, 2)  # 2 decimal places
all_q[all_q == 1] <- 0  # convert ones to zeros    
all_q <- data.frame(all_q)
max(all_q)

all_q

bes <- all_q %>%
  rownames_to_column('gene') %>%
  filter_if(is.numeric, any_vars(. >0.62)) %>%
  column_to_rownames('gene')   # get only questions that have correlation less than 0.3 with all other variables. 

# tau is The normal equivalent of the cutpoints

)
Phi two binary 

, L, C or lambda


if(!require(rcompanion)){install.packages("rcompanion")}
if(!require(psych)){install.packages("psych")}
if(!require(DescTools)){install.packages("DescTools")}

epsilonSquared(questions, group = "row")

Input =(
"Adopt      Always  Sometimes  Never
Size
Hobbiest         0          1      5
Mom-and-pop      2          3      4
Small            4          4      4
Medium           3          2      0
Large            2          0      0
")

Tabla = as.table(read.ftable(textConnection(Input)))

class(Tabla)

SomersDelta(as.data.table(questions)
,
            direction  = "column",
            conf.level = 0.95)


```

```{r}

for(i in 1:ncol(all_q)){
  print(c(i, max(all_q[i])))
}  # get the max correlation across a column 

# however, polychoric correlations are conservative 


trial <- questions %>% 
  dplyr::select(Q26, Q44)

asd <- polychoric(trial)

asd <- tetrachoric(trial)
?polychoric
sum(questions[26] == questions[44])

sp <- cor(questions, method = "spearman")  # 
sp <- round(sp, 2)  # 2 decimal places
sp[sp == 1] <- 0  # convert ones to zeros    
sp <- data.frame(sp)

kn <- cor(questions, method = "kendall")  # 
kn <- round(kn, 2)  # 2 decimal places
kn[kn == 1] <- 0  # convert ones to zeros    
kn <- data.frame(kn)


trial_all <- all_q %>%
  rownames_to_column('gene') %>%
  filter_if(is.numeric, all_vars(. < 0.3)) %>%
  column_to_rownames('gene')   # get only questions that have correlation less than 0.3 with all other variables. 


all_low <- row.names(trial_all) 

foo <- function(){
  a = {1}
for (i in 1:ncol(questions)){
  for (j in 1:ncol (questions)){
    a = c(a, sum(questions[i] == questions[j])) 
    
#return(sum(questions[i] == questions[j]))
  }}
  return (a)
}

a <- foo()

a[a ==950] <- 0

sort(a, decreasing = T)

#polychoric correlation produces more conservative results (produces lower absolute values).

```


### References



