---
title: "survey_irrational-beliefs"
output: word_document
bibliography: library.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, comment = NA,
                      fig.height = 15, fig.width = 15)
#this chunk is used to silence code, warnings, comments and hash from appearing in output.
```


# Statistical analysis 


R packages used in the analysis were psych version 1.8.4 for factor analysis , HH version 3.1-35 for plotting likert data, factorMerger version 0.3.6 for merging factor levels and broom version 0.5.0 for tidying up model coefficients. 


# Results



```{r}
options(scipen = 999)

library(data.table)
library(lavaan)
library(psych)
library(factorMerger) # for post-hoc analysis for MANOVA
library(mvoutlier) # detecting multivariate outliers
library(micompr) # for testing for normality and homogenity of variance in MANOVA
library(mvnormtest)  # for shapiro test of normality for MANOVA
library(table1)
library(tidyverse)
library(FactoMineR)
library(factoextra)
library(tibble)
library(corpcor)
library(GPArotation)
library(psych)
library(stargazer)
library(broom)
library(ggpubr)
require(foreign)
require(ggplot2)
require(MASS)
require(Hmisc)
require(reshape2)
library(knitr)
library(HH) # for plotting likert data
library(citr) # for citation
library(flextable) # nice tables
library(corrr) # for correlation matrix, a really nice package
library(ICSNP)   # for hotelling test
library(readxl)
library(mirt) # for fitting multidimentional IRT 
```


```{r}
# When you are done writing, your bibliography file is likely to contain some unneeded references, which you added while writing but removed during revisions. tidy_bib_file() removes unneeded (or duplicate) entries from your bibliography file.

SurveyData <-read.csv("es_data.csv")

# ------------------------- primary data checks --------------------------------
## complete cases Questions
completeSurveyData=SurveyData[complete.cases(SurveyData[2:62]), ]

## complete cases all
completeSurveyData=SurveyData[complete.cases(SurveyData), ]

gov_total <- read.csv("CCHECancerRegistry-HamzaGovern_DATA_LABELS_2019-10-02_1455.csv")

# correction for some MRNs mistyped 

fir <- str_extract(SurveyData$MRN, "\\d{4}")   

las <- substring(SurveyData$MRN, 5) 

las <- str_pad(las, 4, pad = "0")

SurveyData$MRN <- as.numeric(paste(fir,las, sep = ""))

SurveyData$MRN[SurveyData$MRN == 201059530] <- 20154530

SurveyData$MRN[SurveyData$MRN == 201444590] <- 20144590

SurveyData$MRN[SurveyData$MRN == 20071040] <- 20070104

SurveyData$MRN[SurveyData$MRN == 20800619] <- 20080619

gov_total <- gov_total %>% 
  dplyr::select(MRN, Governorate)

SurveyData <- inner_join( SurveyData ,gov_total)

#ff <- anti_join(ddd, gov_total )


SurveyData$governorate[SurveyData$Governorate %in% c("Behera", "Dakahlya", "Damietta", "Gharbya", "Ismaeilya", "Kafr EL-Sheikh", "Menoufya", "Qalyoubia", "Sharkya")] <- "Lower Egypt"

SurveyData$governorate[SurveyData$Governorate %in% c("Assyout", "Aswan", "Beny Swef", "Luxor", "Menia", "Qena", "Sohag", "Giza", "Fayoum")] <- "Upper Egypt"

SurveyData$governorate[SurveyData$Governorate %in% c("Cairo", "Alexandria" , "Port-Saeid", "Suez")]  <- "Urban Governorates"

SurveyData$governorate[SurveyData$Governorate %in% c("South Sinai", "Red Sea", "North Sinai", "Matrouh", "New Valley"  )]  <- "Frontier Governorates"


SurveyData$governorate[SurveyData$Governorate %in% c("Outside Egypt")]  <- "Outside Egypt"

#table(SurveyData$governorate)

SurveyData$family_income_total = SurveyData$Father_month_salary + SurveyData$Mother_month_salary + SurveyData$other_income_val

SurveyData <- filter(SurveyData, author != "other")

#---------------- recode variables => salary, education ---------------------------
## salary
SurveyData$income_category =ifelse(
  SurveyData$family_income_total <= 2000, " 1_less than 2000",
  ifelse(SurveyData$family_income_total > 2000 & SurveyData$family_income_total <= 6000  ,"2_from 2000 to 6000",
         ifelse(SurveyData$family_income_total > 6000 & SurveyData$family_income_total <= 12000  ,"3_from 6000 to 12000",
                ifelse(SurveyData$family_income_total > 12000 & SurveyData$family_income_total <= 25000  ,"4_from 12000 to 25000",
                       "5_more than 25000"
                )
         )
  )
)


##education

SurveyData$father_edu_recoded =ifelse(
  SurveyData$father_edu == "Illiterate", " 1_Illiterate",
  ifelse(SurveyData$father_edu == "read n write","2_read n write",
         ifelse(SurveyData$father_edu == "primary","3_primary",
                ifelse(SurveyData$father_edu =="prep","4_prep",
                       ifelse(SurveyData$father_edu =="secondary +","4_secondary +",
                              ifelse(SurveyData$father_edu =="University Degree","5_University Degree",
                                     
                                     "Master & PHD"
                              )
                       )
                )
         )
  )
)



library(psych)
library(GPArotation)

# get the mean for all questions (before questions removal)

questions <- SurveyData[, 2:62]

# ---------------------------  combining categorical variables ---------------------------------

# I will combine levels of income category


SurveyData$income_category_comb <- SurveyData$income_category
SurveyData$income_category_comb[SurveyData$income_category_comb == "5_more than 25000"] <- "4_from 12000 to 25000"
SurveyData$income_category_comb[SurveyData$income_category_comb == "4_from 12000 to 25000"] <- "4_more than 12000"


# Now we are having only 4 categories, the smallest contains 22 subjects 

# I will combine father education levels 

SurveyData$father_edu_comb <- SurveyData$father_edu_recoded
SurveyData$father_edu_comb[SurveyData$father_edu_comb == "Master & PHD"] <- "5_University Degree"


SurveyData$father_edu_combined <- as.factor(SurveyData$father_edu_comb)

levels(SurveyData$father_edu_combined) <- c("Illiterate", "R&W_prim", "R&W_prim", "prep/sec", "prep/sec", "University Degree & post-grad")


total_mean <- data.frame(cbind(SurveyData[, 1], rowMeans(questions), SurveyData$father_edu_combined, SurveyData$income_category_comb, SurveyData$sex))
colnames(total_mean) <- c("ID", "Mean_score", "father_education", "income_category", "sex")

SurveyData$mo_edu <- SurveyData$Mother_education

SurveyData$mo_edu <- car::recode(SurveyData$mo_edu, "'Illiterate' = 1; c('read n write', 'primary') = 2; 
  c('prep', 'secondary +') = 3; 'University Degree' = 4")


SurveyData$fa_edu <- car::recode(SurveyData$father_edu_combined, "'Illiterate' = 1; 'R&W_prim' = 2; 'prep/sec' = 3; 'University Degree & post-grad' = 4")

SurveyData$parent_education <- SurveyData$fa_edu

SurveyData$parent_education[SurveyData$author == "mother"] <- SurveyData$mo_edu[SurveyData$author == "mother"]

levels(SurveyData$parent_education) <- c("Illiterate", "R&W-PRIM", "PREP-SEC", "University")

```

Apart from few missing data in demographics (table 1), nothing was missing regarding the survey answers to all questions. We calculated total family income by summing up father monthly salary, mother monthly salary as well as other incomes gained from other resources. We tried to categorize income and father education into meaningful categories, with an attempt to avoid small number of participants in subgroups so as to have valid statistical comparisons.  

```{r}

SurveyData$parent <- SurveyData$author

levels(SurveyData$parent) <- c("father", "mother", "mother")

SurveyData$Number_of_family_members <-  cut(SurveyData$family_members, c(-1, 4, 6, 100), c("<=4", "5-6", ">=7"))
  
table1(~ sex +  governorate + parent + Number_of_family_members +  governorate + parent_education + income_category_comb , data=SurveyData,  topclass="Rtable1-zebra")

```

# proper questions 

```{r}

ques45 <- questions %>% 
  dplyr::select(-c(6,9,12,18,45,3,47, 55,61,44,50, 16,17,49,51, 40))

ques44 <- questions %>% 
  dplyr::select(-c(6,9,10, 11,12,18,45,3,47, 55,44,50, 16,17,49,51, 40))

```

# MIRT

### modern psychometric with R book

##### Dimentionality assessment

```{r}
library("MPsychoR")
library("mirt")
library("Gifi")
dim_ques <- princals(ques45)
plot(dim_ques, main = "Irrational thoughts loadings")


# checking before questions removal 
# dim_ques_61 <- princals(questions)
# plot(dim_ques_61, main = "Irrational thoughts loadings")


```


unidimensionality is violated

### check number of factors 

```{r}
nfactors(ques45, n = 4, cor = "poly")

```

MAP : 2 factors 
VSS : 2 factors 
BIC : 4 factors 


### scree plot

```{R}
Rdep <- polychoric(ques45)$rho
evals <- eigen(Rdep)$values
scree(Rdep, factors = FALSE)

```


```{r}

plot(dim_ques, "screeplot")
```

2 factors 



```{r}

fa.parallel(ques45, cor = "poly")

```

### IFA method 

```{r}
# mirt fits a maximum likelihood (or maximum a posteriori) factor analysis model to any mixture of dichotomous and polytomous data under the item response theory paradigm. Thus I don't have to specify "poly"
mod_graded <- mirt(ques45, 1, itemtype = "graded")
mod_graded2 <- mirt(ques45, 2, itemtype = "graded", TOL = 0.001)

anova(mod_graded, mod_graded2, verbose = T)



mod_graded4 <- mirt(ques45, 4, itemtype = "graded", TOL = 0.001)
anova(mod_graded2, mod_graded2, verbose = T)

```
AIC BIC and log likelihood changed dramatically when using 2 factors instead of 1 

this is also true when comparing two and four factors, I may consider.. However, maybe this is because of the large sample size 




##### I have chosen this method since Dave suggested using M2 

```{r}

mod_gpcm <- mirt(ques45, 2, itemtype = "gpcm")
# mod_pc <- mirt(ques45, 2, itemtype = "PC3PL") Partially compensatory models can only be estimated within a confirmatory model
#mod_ggum <- mirt(ques45, 2, itemtype = "ggum")  not used. check MIRT word document 
#mod_seq <- mirt(ques45, 2, itemtype = "sequential", TOL = 0.001) # not used, check MIRT word document
#mod_spline <- mirt(ques45, 2, itemtype = "spline") # no restrictions on the model regarding maths, over sophisticated 

stat_graded2 <- M2(mod_graded2) # didn't use type C2 since I have too many dfs 
stat_gpcm <- M2(mod_gpcm)
#stat_ggum <- M2(mod_ggum , type = 'C2')
#stat_seq <- M2(mod_seq , type = 'C2')
#stat_spline <- M2(mod_spline , type = 'C2')

```

as per the modern psychometrics book 
For the CFI, we can use the same 0.95 fit
cutoff as in CFA/SEM. For the RMSEA, the CFA/SEM cutoff was 0.05 for a good
fitting model. In IRT, it is suggested to use 0.05/k (with k being the number of
categories per item) as fit cutoff.


https://groups.google.com/forum/#!topic/mirt-package/gy63tCz2W98   ( same as SEM, 0.9 for CFI and 0.05 for RMSEA)

Do you have a large sample? If so could be that small differences in expected vs observed response freqs are causing significant p values even though magnitude of misfit is not that big per empirical item plots (and S-X2 RMSEA) https://groups.google.com/forum/#!searchin/mirt-package/M2|sort:date/mirt-package/ZWFimGtUEsI/MXx6pXoaBgAJ 

 
### item fit 

##### graded 

```{r}
ifit_ques45_graded <- mirt::itemfit(mod_graded2)

ifit_ques45_graded[ifit_ques45_graded[, 5] < 0.05, ] ## misfitting items

```

##### gpcm 

```{r}
ifit_ques45_gpcm <- mirt::itemfit(mod_gpcm)

ifit_ques45_gpcm[ifit_ques45[, 5] < 0.05, ] ## misfitting items

```


```{r}
summary(mod_graded2, rotate = "oblimin")

```

```{R}
itemplot(mod_graded2, 3, main = "ICS addit3",
rot = list(xaxis = -70, yaxis = 50, zaxis = 10))

```

```{r}
coef(mod_graded2)

```
### multidimentional item location 

```{r}
head(MDIFF(mod_graded2))

```

### person paramegters 

```{r}

head(fscores(mod_graded2))

```

### residuals

```{r}

res <- residuals(mod_graded2, type = "LDG2", digits = 4, df.p = T)

```



# exploratory multigroup MIRT

### for parent 

```{r}

class2 <- SurveyData$parent
levels(class2)
modMG <- multipleGroup(ques45, model = 2, group = class2,
SE = TRUE, itemtype = "graded", TOL = 0.01)  # model =2 means two dimentional model
# https://groups.google.com/forum/#!searchin/mirt-package/multiplegroup$20exploratory|sort:date/mirt-package/j-ZyjuXPNcY/zoSTk-BEBgAJ 

# which p-value to use ?  Ans: 0.05, but u can use 0.01 
# https://groups.google.com/forum/#!searchin/mirt-package/multiplegroup$20exploratory|sort:date/mirt-package/CXWkXsKCBvo/WAivQ5ABBQAJ

astiDIF <- DIF(modMG, c('a1', 'd'), Wald = TRUE,
p.adjust = 'fdr')
round(astiDIF$adj_pvals[astiDIF$adj_pvals < 0.05], 4)
```

no DIF items 

### parent education 

```{r}

class3 <- SurveyData$parent_education
levels(class3) <- c("ILL", "rwp", "psec", "uni")

modMG_edu <- multipleGroup(ques45, model = 2, group = class3, itemtype = "graded", TOL = 0.01, SE = T)  

DIF_edu <- DIF(modMG_edu, c('a1', 'd'), Wald = T,
p.adjust = 'fdr', simplify = T, verbose = T)
round(DIF_edu$adj_pvals[astiDIF$adj_pvals < 0.05], 4)



```

### income category 

```{r}

class4 <- factor(SurveyData$income_category)
modMG_inc <- multipleGroup(ques45,  model = 2, group = class4, itemtype = "graded", TOL = 0.01, SE= T)

DIF_inc <- DIF(modMG_inc, c('a1', 'd'), Wald = TRUE,
p.adjust = 'fdr')
round(DIF_inc$adj_pvals[astiDIF$adj_pvals < 0.05], 4)


```


### Number of children 

```{r}

class5 <- factor(SurveyData$Number_of_family_members)
modMG_mem <- multipleGroup(ques45,  model = 2, group = class5, itemtype = "graded", TOL = 0.01, SE= T)

DIF_mem <- DIF(modMG_mem, c('a1', 'd'), Wald = TRUE,
p.adjust = 'fdr')
round(DIF_mem$adj_pvals[astiDIF$adj_pvals < 0.05], 4)

```



# TRY after removal 2 questions

```{R}
ques43_irt <- ques45 %>% 
  dplyr::select(- c(40,44))

mod_graded2_43 <- mirt(ques43_irt, 2, itemtype = "graded")

stat_graded_43 <- M2(mod_graded2_43) # didn't use type C2 since I have too many dfs 

```

# Try after removal 20 questions 

```{r}
ques19_irt <- ques45 %>% 
  dplyr::select(- c(20:61))

mod_graded2_19 <- mirt(ques19_irt, 1, itemtype = "graded")

stat_graded_19 <- M2(mod_graded2_19) # didn't use type C2 since I have too many dfs 

```


```{r}

plot(mod_graded2, type = "info", main = "Item Information")

# This plot (not shown here) tells us in which trait area our entire scale is
# informative and thus able to assess a person’s location on the conservatism trait
# with good precision

```


### Limitations

We have used only three levels of likert scale questions. Results could have been more powerful if we used at least five levels. In addition, using an odd number of levels may drive the responder to choose the middle response. On the contrary, using an even number of levels could have avoided this. 





# Supplementary figures 

### likert plot for demand factor

```{r fig.height= 15, fig.width= 15}
#https://cran.r-project.org/web/packages/HH/HH.pdf

lkrt_plot <- function(factor, name){
  
  fct <- data.frame(cbind(SurveyData[1], factor ))
  
  fct <- gather(fct, "question", "response", -ID)
  
  g <- fct %>% group_by(question, response) %>% 
    summarise(n = n()) %>% 
    arrange(response)
  
  g <- reshape2::melt(g, id.vars=c("question", "response"))
  names(g)[3] <- "Agreement"
  
  g$response <- as.factor(g$response)
  
  levels(g$response) <- c("never", "sometimes", "common")
  
  return(likert(question ~ response , value="value", data=g,
         main = name,
         as.percent = T,
         ylab=NULL,
         scales=list(y=list(relation="free")), layout=c(1,2)))
  
}


```


We checked whether irrational believes scores vary according to parent or child factors. We applied multivariate analysis of variance instead of univariate ANOVA or t-test to avoid family-wise error rate(alpha inflation). In addition, this enables us to preserve power since scale scores are correlated. 



Parametric tests as MANOVA should be used with caution with ordinal data like the likert scale we are dealing with, as assumptions maybe violated. However, we are using the average score for the questions. This renders the questions in a continuous rather than a categorical form. The same methodology has been previously applied in a similar article written by [@bonnerDevelopmentValidationParent2006]. Moreover, with the large sample size we have, normality is never a problem. [click to view reference](https://www.st-andrews.ac.uk/media/capod/students/mathssupport/OrdinalexampleR.pdf). Box's M test was used to test the assumption of homogeneity of variances and covariances and the assumption was not violated.  

```{r}

# MANOVA for gender

# Assumptions of MANOVA

# MANOVA can be used in certain conditions:
# 
# The dependent variables should be normally distribute within groups. The R function mshapiro.test( )[in the mvnormtest package] can be used to perform the Shapiro-Wilk test for multivariate normality. This is useful in the case of MANOVA, which assumes multivariate normality.
# 
# Homogeneity of variances across the range of predictors.
# 
# Linearity between all pairs of dependent variables, all pairs of covariates, and all dependent variable-covariate pairs in each cell

library(RVAideMemoire)

#mshapiro.test(cor_data)

# here I tried to test for multivariate normality, however, due to the large sample size, p value of shapiro wilk test is definetely significant. Thus, we will shift to the package MVN: An R Package for Assessing Multivariate Normality  

library(MVN)

# result <- mvn(data = cor_data, mvnTest = "mardia")
# result$multivariateNormality
# 
# 
# result <- mvn(data = cor_data, mvnTest = "hz")
# result$multivariateNormality
# 
# 
# result <- mvn(data = cor_data, mvnTest = "royston")
# result$multivariateNormality
# # this depends on shapiro, so shouldn't be used with a large sample size
# 
# 
# result <- mvn(data = cor_data, mvnTest = "dh")
# result$multivariateNormality
# 
# 
# result <- mvn(data = cor_data, mvnTest = "energy")
# result$multivariateNormality

# here it appears that for all tests, the normality assumption is violated, however, this is not a problem because we have a large sample size. 


# mnva_qplot <- function(data){
#   # create univariate Q-Q plots
# mvn(data = data, mvnTest = "royston", univariatePlot = "qqplot")
# }
# 
# dsc_table <- mnva_qplot(cor_data)
# 
# dsc_table <- dsc_table$Descriptives[1:8]
# 
# rmdtable(dsc_table)

# mnva_hist <- function(data){   # create univariate histograms
# 
#   mvn(data = cor_data, mvnTest = "royston", univariatePlot = "histogram")
# }
# 
# mnva_hist(cor_data)


# From the plots, it appears that demand factor is the most problematic. 
# However, again this is not a problem

# https://cran.r-project.org/web/packages/MVN/vignettes/MVN.pdf  the perfect package


# Now it is time to test for homogenity of variance assumption 

 # create a data set for three mult columns and sex 

test_data <- cbind(cor_data, SurveyData$sex)

colnames(test_data)[4] <- "sex"

library(micompr)

#assumptions_manova(cor_data, test_data$sex)

# Box's M test is a multivariate statistical test used to check the equality of multiple variance-covariance matrices.[1] The test is commonly used to test the assumption of homogeneity of variances and covariances in MANOVA and linear discriminant analysis. It is named after George E. P. Box.
# 
# Box's M test is susceptible to errors if the data does not meet model assumptions or if the sample size is too large or small.[2] Box's M test is especially prone to error if the data does not meet the assumption of multivariate normality


# here it appears that multivariate homogenity of variance assumption is not violated


################

# datacamp made a quick reference for MANOVA with assumptions 

# I will move with this guide step by step 

# https://www.statmethods.net/stats/anova.html 

mnv_tbl <- function(col){
  
  fit_sex <- tidy(manova(as.matrix(cor_data) ~ col), test = "Hotelling-Lawley")

fit_sex <- fit_sex[1, c(1,2,3,4,7)]

colnames(fit_sex) <- c("Factor", "df", "Hotelling-Lawley trace", "F", "p.value")

return(fit_sex)  
}

mnv_sex <- mnv_tbl(SurveyData$sex)

# fit_sex <- tidy(manova(as.matrix(cor_data) ~ SurveyData$sex, test = "Hotelling-Lawley"), test = "Hotelling-Lawley")
# 
# fit_sex <- fit_sex[1, c(1,2,3,4,7)]
# 
# colnames(fit_sex) <- c("Factor", "df", "Hotelling-Lawley trace", "F", "p.value")


# for gender, we have 1 degree of freedom, so all tests should be identical. 
# however, here wilks is different and I don't know why 

# summary(fit, test="Pillai")
# 
# summary(fit, test="Wilks")
# 
# summary(fit, test="Hotelling-Lawley")
# 
# summary(fit, test="Roy")


# When the hypothesis degrees of freedom, h, is one, all four test statistics will lead to identical results. When h>1,
# the four statistics will usually lead to the same result. When they do not, the following guidelines from
# Tabachnick (1989) may be of some help.
# Wilks' Lambda, Lawley's trace, and Roy's largest root are often more powerful than Pillai's trace if h>1 and one
# dimension accounts for most of the separation among groups. Pillai's trace is more robust to departures from
# assumptions than the other three.
# Tabachnick (1989) provides the following checklist for conducting a MANOVA. We suggest that you consider
# these issues and guidelines carefully.

# significant result, and it is the same value for all tests

#summary.aov(fit, test="Wilks")  # univariate anova

# checking assumptions

# The aq.plot() function in the mvoutlier package allows you to identfy multivariate outliers by plotting the ordered squared robust Mahalanobis distances of the observations against the empirical distribution function of the MD2i. Input consists of a matrix or data frame. The function produces 4 graphs and returns a boolean vector identifying the outliers.
library(mvoutlier)

# outliers <- aq.plot(cor_data)
# outliers # show list of outliers

# testing normality 

# mshapiro.test(as.matrix(cor_data))
# 
# # Graphical Assessment of Multivariate Normality
# x <- as.matrix(cor_data) # n x p numeric matrix
# center <- colMeans(x) # centroid
# n <- nrow(x); p <- ncol(x); cov <- cov(x); 
# d <- mahalanobis(x,center,cov) # distances 
# qqplot(qchisq(ppoints(n),df=p),d,
#   main="QQ Plot Assessing Multivariate Normality",
#   ylab="Mahalanobis D2")
#abline(a=0,b=1)

```



```{r}

# sex_manova <- rmdtable(tidy(HotellingsT2(as.matrix(cor_data) ~ SurveyData$sex)))

# SPSS will give Hotelling's Trace, and it has to convert to Hotelling's T^2 as follows:
#   Multiplying Hotelling's Trace by (N - L), where N is the sample size across all groups and L is the number of groups, gives a generalized version of Hotelling's T^2.

# https://www.researchgate.net/post/How_can_I_do_Hotellings_T-square 

#Parent score differ according to child's gender. 

```



```{r}

### MANOVA father education 

# SurveyData$mo_edu <- SurveyData$Mother_education
# 
# SurveyData$mo_edu <- car::recode(SurveyData$mo_edu, "'Illiterate' = 1; c('read n write', 'primary') = 2; 
#   c('prep', 'secondary +') = 3; 'University Degree' = 4")
# 
# 
# SurveyData$fa_edu <- car::recode(SurveyData$father_edu_combined, "'Illiterate' = 1; 'R&W_prim' = 2; 'prep/sec' = 3; 'University Degree & post-grad' = 4")
# 
# SurveyData$parent_education <- SurveyData$fa_edu
# 
# SurveyData$parent_education[SurveyData$author == "mother"] <- SurveyData$mo_edu[SurveyData$author == "mother"]
# 
# levels(SurveyData$parent_education) <- c("Illiterate", "R&W-PRIM", "PREP-SEC", "University")
# 
edu_data <- cbind(cor_data, SurveyData$parent_education)

colnames(edu_data)[4] <- "edu"

edu_data <- edu_data[complete.cases(edu_data),]

#assumptions_manova(edu_data[1:3], as.factor(edu_data$edu))

# here the homogenity of variance assumption is not violated
#although the test is sensitive to large numbers, meaning that it may cause the assumption to be violated even when tiny departures from hokmogenity are present. 
#however, here it is not violated.

fit_edu <- manova(as.matrix(cor_data) ~ SurveyData$parent_education)

m <- list(summary(fit_edu, test="Pillai"),
summary(fit_edu, test="Wilks"),
summary(fit_edu, test="Hotelling-Lawley"),
summary(fit_edu, test="Roy"),
summary.aov(fit_edu, test="Wilks")  # univariate anova
)

mnv_edu <- mnv_tbl(SurveyData$parent_education)

```

```{r}

### MANOVA for income

inc_data <- cbind(cor_data, SurveyData$income_category_comb)

colnames(inc_data)[4] <- "inc"
inc_data <- inc_data[complete.cases(inc_data),]

#assumptions_manova(inc_data[1:3], as.factor(inc_data$inc))

# homogenity of variance assumption is not violated

mnv_income <- mnv_tbl(SurveyData$income_category_comb)

fit <- manova(as.matrix(inc_data[1:3]) ~ inc_data$inc)
#tidy(fit, test = "Hotelling-Lawley")

m <- list(summary(fit, test="Pillai"),
summary(fit, test="Wilks"),
summary(fit, test="Hotelling-Lawley"),
summary(fit, test="Roy"),
summary.aov(fit, test="Wilks")  # univariate anova
)

 # it is highly significant for all

```


```{r}

# ANOVA parent gender

parent_data <- cbind(cor_data, SurveyData$author)

colnames(parent_data)[4] <- "parent"

parent_data <- parent_data[complete.cases(parent_data),]

#assumptions_manova(parent_data[1:3], as.factor(parent_data$parent))

#assumption not working and I don't know why

#assumptions_manova(parent_data[1:3], parent_data$parent)

# homogenity of variance assumption is not violated

mnv_parent <- mnv_tbl(SurveyData$parent_education)
# results of the function are wrong 

fit_parent <- manova(as.matrix(parent_data[1:3]) ~ parent_data$parent)

fit_parent <- tidy(fit_parent, test = "Hotelling-Lawley")

m <- list(summary(fit, test="Pillai"),
summary(fit, test="Wilks"),
summary(fit, test="Hotelling-Lawley"),
summary(fit, test="Roy"),
summary.aov(fit, test="Wilks")  # univariate anova
)

fit_parent <- fit_parent[1, c(1,2,3,4,7)]

colnames(fit_parent) <- c("Factor", "df", "Hotelling-Lawley trace", "F", "p.value")

mnv_tbls <- rbind(mnv_sex, fit_parent, mnv_edu, mnv_income )
 # it is highly significant for all

mnv_tbls$Factor <- c("child gender", "parent gender", "parent education", "family income")

mnv_tbls <- rmdtable(mnv_tbls)

mnv_tbls <- color(mnv_tbls, i = ~ p.value < 0.05, j = ~ p.value , color = "red")
mnv_tbls
```
Table MANOVA results for parent and children factors

The scale scores varied significantly. with child gender, family income and parent education but not with parent gender. For child gender and parent gender, the degrees of freedom were 1. For all results, the four tests, Pillai, Wilks, Hotelling-Lawley and Roy provided quite similar results and here we report Hotelling-Lawley trace results. 


### Post-hoc MANOVA family income


```{r}

levels(inc_data$inc) <- c("<2", "2-6", "6-12", ">12")

inc_post <- mergeFactors(response = inc_data[,1:3],
                                   factor = factor(inc_data$inc),
                                   method = "adaptive") 

plot(inc_post, panel = "GIC", nodesSpacing = "effects", colorCluster = TRUE)

```

The higher the family income, the less common irrational believes are. 

### Domains' Averages according to income group 


```{r}

inc_mean <- inc_data %>% 
  group_by(inc) %>%
  summarise_all(mean)

rmdtable(inc_mean)
```


### Significance of groups splitting 


```{r}

inc_post_h <- mergingHistory(inc_post, showStats = TRUE) 
inc_post_h <- rmdtable(inc_post_h)

inc_post_h <- color(inc_post_h, i = ~ pvalVsPrevious < 0.05, j = ~ pvalVsPrevious , color = "red")

inc_post_h

```

Each row of the above frame describes one step of the merging algorithm. First two columns specify which groups were merged in the iteration. Last two columns are p-values for the Likelihood Ratio Test - against the full model (pvalVsFull) and against the previous one (pvalVsPrevious). Only the last step is significant, splitting the income categories into less than 6000 and more than 6000. 


### Post hoc MANOVA parent education level


```{r fig.height= 12, fig.width= 12}

library(factorMerger)

# post hoc MANOVA education

edu_post <- mergeFactors(response = edu_data[,1:3],
                                   factor = factor(edu_data$edu),
                                   method = "adaptive") 

plot(edu_post, panel = "GIC", nodesSpacing = "effects", colorCluster = TRUE)

```

Parents with a university degree or higher experience less commonly irrational believes than others with lower educational level. 


### Domains' Averages according to income group 


```{r}

edu_mean <- edu_data %>% 
  group_by(edu) %>%
  summarise_all(mean)

rmdtable(edu_mean)

```

### Significance of groups splitting 


```{r}

edu_post_h <- mergingHistory(edu_post, showStats = TRUE) 
edu_post_h <- rmdtable(edu_post_h)

edu_post_h <- color(edu_post_h, i = ~ pvalVsPrevious < 0.05, j = ~ pvalVsPrevious , color = "red")

edu_post_h
# data_edu <- SurveyData

# data_edu <- filter(data_edu, !is.na(fa_edu), !is.na(mo_edu))
# 
# data_edu$max_edu <- pmin(data_edu$fa_edu, data_edu$mo_edu)
# 
# n <- data.frame(cbind(data_edu$fa_edu, data_edu$mo_edu)) # I am using this step, because pmax is producing wrong results in the large dataframe
# 
# n$max <- pmax(n$X1, n$X2)
# 
# data_edu$max_edu <- n$max
# # table(n$max)
# table(data_edu$max_edu)


```

Splitting university from all other educational levels yielded significant results. 



```{r}

# We found that score differs significantly for the total questions. However, bear is the only factor where sex is found to be significantly affecting the score. 


# demand_p <- var.test(demand ~ sex, data = dat.clean)
# 
# demand_p <- demand_p$p.value
# 
# bear_p <- var.test(bear ~ sex, data = dat.clean)
# bear_p <- bear_p$p.value
# 
# accuse_p <- var.test(accuse ~ sex, data = dat.clean)
# accuse_p <- accuse_p$p.value
# 
# cbind(demand_p, bear_p, accuse_p)
# # combining box plots 

# # ggboxplot(dat.clean, x = "father_edu_combined", y = "cognitive_dim", 
# #           color = "income_category_comb",
#           ylab = "Mean score", xlab = "Income category")

```


### References
