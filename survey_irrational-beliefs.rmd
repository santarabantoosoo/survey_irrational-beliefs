---
title: "survey_irrational-beliefs"
output: html_notebook
bibliography: library.bib
---

# to do: 

remove libraries not needed

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, comment = NA)
#this chunk is used to silence code, warnings, comments and hashes. 
```

Division of the paper 

mention item response theory in the introduction. why it is important. 

# Items that I suggest adding to the introduction 

Arguably, one of the more relevant IRT models to health-based research is the graded
response model [@samejima1997; @samejima1969]

# Statistical analysis 


R Foundation for Statistical Computing, Vienna, Austria version 3.6.0 was used for the statistical analysis. Multidimensional item response theory package version 1.31 [@chalmersMirtMultidimensionalItem2012] using Unconditional maximum likelihood estimator with expectation–maximization (EM) algorithm was used for the analysis. An exploratory multidimensional extension of the graded response model [@samejima1997; @samejima1969] was used to model the data. Exclusions of questions from the tool was based both on principal investigator opinion, factor loading as well as item fit tests. M2 fit statsitics were used to assess model fit and ANOva was used to compare different models' performance. Other R packages used were HH version 3.1-35 for plotting likert data and 

# Results

The irrational beliefs tool contains three response options per question and would
therefore have two thresholds (difficulty parameters) estimated for each item. The difficulty
parameter lends insight into the relationship between the latent trait (e.g. level of irrational thoughts) and the response categories chosen for the items. For  more technical
details of the graded response model, please see the online appendix.

Online appendix include two Rmarkdown files. The first one is a step by step exclusion of questions with justification till we reach the 43 questions. The second file includes detailed assessment of our multidimension graded response model containing the 43 questions. 

```{r}
options(scipen = 999)
options("mc.cores"=2) # choosing the number of cores that optimizes performance. 

library(finalfit)# for the descriptive table
library(packrat) # for detecting unused packages. But I failed
library(data.table)
library(lavaan)
library(psych)
library(factorMerger) # for post-hoc analysis for MANOVA
library(mvoutlier) # detecting multivariate outliers
library(micompr) # for testing for normality and homogenity of variance in MANOVA
library(mvnormtest)  # for shapiro test of normality for MANOVA
library(tidyverse)
library(FactoMineR)
library(factoextra)
library(tibble)
library(corpcor)
library(GPArotation)
library(psych)
library(stargazer)
library(broom)
library(ggpubr)
require(foreign)
require(ggplot2)
require(MASS)
require(Hmisc)
require(reshape2)
library(knitr)
library(HH) # for plotting likert data
library(citr) # for citation
library(flextable) # nice tables
library(corrr) # for correlation matrix, a really nice package
library(ICSNP)   # for hotelling test
library(readxl) # for importing data
library(mirt) # for fitting multidimentional IRT 

names(SurveyData)
#### functions ###

rmdtbl <- function(df){
  # using flextable package to produce customized elegant tables directly into microsoft word
  tbl_alpha <- autofit(theme_vanilla(flextable(df)))
  tbl_alpha <- bg(tbl_alpha, bg = "blue", part = "header")
  tbl_alpha <- color(tbl_alpha, color = "white", part = "header")
  bes <- align(tbl_alpha, align = "center")
  bes <- align_text_col(bes, align = "center")
  return(bes)
}

```


```{r}

SurveyData <-read.csv("es_data.csv")

gov_total <- read_csv("HamzaGovern.csv")

# correction for some MRNs mistyped 

fir <- str_extract(SurveyData$MRN, "\\d{4}")   

las <- substring(SurveyData$MRN, 5) 

las <- str_pad(las, 4, pad = "0")

SurveyData$MRN <- as.numeric(paste(fir,las, sep = ""))

SurveyData$MRN[SurveyData$MRN == 201059530] <- 20155330

SurveyData$MRN[SurveyData$MRN == 201445390] <- 20145390

SurveyData$MRN[SurveyData$MRN == 20071040] <- 20070104

SurveyData$MRN[SurveyData$MRN == 20800619] <- 20080619

gov_total <- gov_total %>% 
  dplyr::select(MRN, Governorate)

SurveyData <- left_join(SurveyData ,gov_total)

#ff <- anti_join(ddd, gov_total )


SurveyData$governorate[SurveyData$Governorate %in% c("Behera", "Dakahlya", "Damietta", "Gharbya", "Ismaeilya", "Kafr EL-Sheikh", "Menoufya", "Qalyoubia", "Sharkya")] <- "Lower Egypt"

SurveyData$governorate[SurveyData$Governorate %in% c("Assyout", "Aswan", "Beny Swef", "Luxor", "Menia", "Qena", "Sohag", "Giza", "Fayoum")] <- "Upper Egypt"

SurveyData$governorate[SurveyData$Governorate %in% c("Cairo", "Alexandria" , "Port-Saeid", "Suez")]  <- "Urban Governorates"

SurveyData$governorate[SurveyData$Governorate %in% c("South Sinai", "Red Sea", "North Sinai", "Matrouh", "New Valley"  )]  <- "Frontier Governorates"

SurveyData$governorate[SurveyData$Governorate %in% c("Outside Egypt")]  <- "Outside Egypt"


SurveyData$family_income_total = SurveyData$Father_month_salary + SurveyData$Mother_month_salary + SurveyData$other_income_val

SurveyData <- filter(SurveyData, author != "other")

#---------------- recode variables => salary, education ---------------------------
## salary
SurveyData$income_category =ifelse(
  SurveyData$family_income_total <= 2000, " 1_less than 2000",
  ifelse(SurveyData$family_income_total > 2000 & SurveyData$family_income_total <= 6000  ,"2_from 2000 to 6000",
         ifelse(SurveyData$family_income_total > 6000 & SurveyData$family_income_total <= 12000  ,"3_from 6000 to 12000",
                ifelse(SurveyData$family_income_total > 12000 & SurveyData$family_income_total <= 25000  ,"4_from 12000 to 25000",
                       "5_more than 25000"
                )
         )
  )
)


##education

SurveyData$father_edu_recoded =ifelse(
  SurveyData$father_edu == "Illiterate", " 1_Illiterate",
  ifelse(SurveyData$father_edu == "read n write","2_read n write",
         ifelse(SurveyData$father_edu == "primary","3_primary",
                ifelse(SurveyData$father_edu =="prep","4_prep",
                       ifelse(SurveyData$father_edu =="secondary +","4_secondary +",
                              ifelse(SurveyData$father_edu =="University Degree","5_University Degree",
                                     
                                     "Master & PHD"
                              )
                       )
                )
         )
  )
)



library(GPArotation)

# get the mean for all questions (before questions removal)

questions <- SurveyData[, 2:62]

# ---------------------------  combining categorical variables------------------------------

# I will combine levels of income category


SurveyData$income_category_comb <- SurveyData$income_category

SurveyData$income_category_comb[SurveyData$income_category_comb == "5_more than 25000"] <- "4_from 12000 to 25000"
SurveyData$income_category_comb[SurveyData$income_category_comb == "4_from 12000 to 25000"] <- "4_more than 12000"

# Now we are having only 4 categories, the smallest contains 22 subjects 

# I will combine father education levels 

SurveyData$father_edu_comb <- SurveyData$father_edu_recoded
SurveyData$father_edu_comb[SurveyData$father_edu_comb == "Master & PHD"] <- "5_University Degree"

SurveyData$father_edu_combined <- as.factor(SurveyData$father_edu_comb)

levels(SurveyData$father_edu_combined) <- c("Illiterate", "R&W_prim", "R&W_prim", "prep/sec", "prep/sec", "University Degree & post-grad")

SurveyData$mo_edu <- SurveyData$Mother_education

SurveyData$mo_edu <- car::recode(SurveyData$mo_edu, "'Illiterate' = 1; c('read n write', 'primary') = 2; 
  c('prep', 'secondary +') = 3; 'University Degree' = 4")

SurveyData$fa_edu <- car::recode(SurveyData$father_edu_combined, "'Illiterate' = 1; 'R&W_prim' = 2; 'prep/sec' = 3; 'University Degree & post-grad' = 4")

SurveyData$parent_education <- SurveyData$fa_edu

SurveyData$parent_education[SurveyData$author == "mother"] <- SurveyData$mo_edu[SurveyData$author == "mother"]

levels(SurveyData$parent_education) <- c("Illiterate", "R&W-PRIM", "PREP-SEC", "University")

```

Apart from few missing data in demographics (table 1), nothing was missing regarding the survey answers to all questions. We calculated total family income by summing up father monthly salary, mother monthly salary as well as other incomes gained from other resources. We tried to categorize income and parent education into meaningful categories, with an attempt to avoid small number of participants in subgroups. 

```{r}

SurveyData$parent <- SurveyData$author

levels(SurveyData$parent) <- c("father", "mother", "mother")

SurveyData$Number_of_family_members <-  cut(SurveyData$family_members, c(-1, 4, 6, 100), c("<=4", "5-6", ">=7"))

SurveyData$parent_education <- SurveyData$parent_education %>% 
  fct_recode("Literate - Primary education" = "R&W-PRIM") %>% 
  fct_recode("Preparatory - Secondary edication" = "PREP-SEC")

SurveyData$income_category_comb <- SurveyData$income_category_comb %>% 
  fct_recode("<= 2000 EGP" = " 1_less than 2000") %>% 
  fct_recode("2000-6000 EGP" = "2_from 2000 to 6000") %>% 
  fct_recode("6000-12000 EGP" = "3_from 6000 to 12000") %>% 
  fct_recode("> 12000 EGP" = "4_more than 12000")

exp = c("sex", "parent", "parent_education", "Number_of_family_members", "income_category_comb", "governorate")

SurveyData <- SurveyData %>% 
  mutate_at(c("sex", "parent", "parent_education", "Number_of_family_members", "income_category_comb", "governorate"), as.factor)

SurveyData %>% 
  mutate(
    sex = ff_label(sex, "Child gender"),
    parent = ff_label(parent, "Relation to patient"),
    parent_education = ff_label(parent_education, "Parent education"),
    Number_of_family_members = ff_label(Number_of_family_members, "Number of family members"),
    income_category_comb = ff_label(income_category_comb, "Income"),
    governorate = ff_label(governorate, "Governorate")
  ) %>% 
  summary_factorlist(explanatory = exp, na_include = T, column = T,  p = F, label = T) -> desc_tab

names(desc_tab)[1:3] <- c("Factor", "Levels", "Frequency (%)")

rmdtbl(desc_tab)

```

Table 1 Sociodemographic characteristics of participants. 


```{r}
tab2 <- SurveyData %>% 
  group_by(parent, parent_education) %>% 
  summarise(Percentage = round(n()/ nrow(SurveyData) * 100 , 2)) 

names(tab2) <- c("Parent", "Parent education", "Percentage")

rmdtbl(tab2)

```
Table 2 Educational level of survey respondents

More than 65% of participants are mothers with at least secondary level education. 



# proper questions 

```{r}

ques53 <- questions %>% 
  dplyr::select(-c(7, 12, 23, 30, 9, 47, 16, 49))

ques52 <- questions %>% 
  dplyr::select(-c(7, 12, 23, 30, 9, 47, 16, 49, 51))

ques51 <- questions %>% 
  dplyr::select(-c(7, 12, 23, 30, 9, 47, 16, 49, 51, 11))

ques49 <- questions %>% 
  dplyr::select(-c(7, 12, 23, 30, 9, 47, 16, 49, 51, 11, 25, 40))

ques48 <- questions %>% 
  dplyr::select(-c(7, 12, 23, 30, 9, 47, 16, 49, 51, 11, 25, 40, 46)) # I will keep 46 

ques49rep <- questions %>% 
  dplyr::select(-c(7, 12, 45, 30, 9, 47, 16, 49, 51, 11, 25, 40))   # replaced 23 with 45

ques48_2 <- questions %>% 
  dplyr::select(-c(7, 12, 23, 30, 9, 47, 16, 49, 51, 11, 25, 40, 45)) # I will keep 46, remove both 45 and 23 

ques48_3 <- questions %>% 
  dplyr::select(-c(7, 12, 23, 30, 9, 47, 16, 49, 51, 11, 25, 40, 41))# removed 41 

ques48_4 <- questions %>% 
  dplyr::select(-c(7, 12, 23, 30, 9, 47, 16, 49, 51, 11, 25, 40, 26))# removed 41 

ques48_5 <- questions %>% 
  dplyr::select(-c(7, 12, 23, 30, 9, 47, 16, 49, 51, 11, 25, 40, 59))# removed 41 

# a new start with 2 domains only 

#The reason is: a) low reliability in the third domain 
#b) few questions in the third domain ( 9 or 10) 
#c) change from 1 domain to 2 is dramatic, but change from three to four is equal to the change of a single question
#d) result is obvious from the scree plot

```

# MIRT

### modern psychometric with R book

##### Dimentionality assessment

Stout et al. (1996) suggested that D(P) ≤ 0.1 reflects that the data are essentially unidimensional while D(P) ≤ 1.0 indicates the presence of “sizable dimensionality” (p. 348). Roussos and Ozbek (2006) constructed a more precise yardstick for the DETECT index, with proposed hatch marks for essential unidimensionality/weak multidimensionality (D(P) ≤ 0.2), weak to moderate multidimensionality (0.2 < D(P) ≤ 0.4, moderate to large multidimensionality (0.4 < D(P) ≤ 1.0), and strong multidimensionality (D(P) > 1.0). 

```{r}
expl.detect(ques43, score, 2)

conf.detect(ques43, score, 2)

# check that these items are like what u have got through the other method. 



```

# The _score_ argument is an ability estimate (e.g., summed score, mean score, or IRT-scaled score)._nclusters_ 
# indicates the number of hypothesized clusters, just as in exploratory factor analysis, while _itemcluster_
# specifies the associated cluster of each item. Run ?expl.detect, ?conf.detect, or ?detect.index for further 
# information on running the DETECT procedure and interpreting the results.



```{r}
library("MPsychoR")
library("mirt")
library("Gifi")
dim_ques <- princals(ques53)
plot(dim_ques, main = "Irrational thoughts loadings")

```

unidimensionality is violated

### check number of factors 

```{r}
no_fact <- nfactors(ques53, n = 4, cor = "poly", rotate = "oblimin") # we set the max number of factors to 4
# vss = 2
# MAP = 4
# emperical BIC = 4
# ss adjusted BIC = 4

```

```{r}
resvss <- vss(ques53, n.obs = nrow(ques53), plot = FALSE, rotate = "oblimin")

# VSS complexity 1 achieves a maximimum of 0.62  with  1  factors
# VSS complexity 2 achieves a maximimum of 0.72  with  2  factors
# 
# The Velicer MAP achieves a minimum of 0  with  2  factors 
# BIC achieves a minimum of  -5626.4  with  3  factors
# Sample Size adjusted BIC achieves a minimum of  -1942.75  with  5  factors
```


```{r}

#trial one dimension
# 
# faques53_one <- fa(ques53, 1, cor = "poly")
# summary(faques53_one)
# # very very low TLI 
# 
# faques53_two <- fa(ques53, 2, cor = "poly")
# summary(faques53_two)
# 
# 
# # improved estimates 
# # TLI : 0.625
# 
# faques53_three <- fa(ques53, 3, cor = "poly")
# summary(faques53_three)
# 
# # TLI : 0.646
# 
# faques53_four <- fa(ques53, 4, cor = "poly")
# summary(faques53_four)
# 
# TLI : 0.664

# However, all of these don't fit the model .. IRT is the choice


```


### scree plot

```{R}
Rdep <- polychoric(ques53)$rho
evals <- eigen(Rdep)$values
scree(Rdep, factors = FALSE)

```

It seems obvious from the scree plot that two factors is a better option. 

```{r}

plot(dim_ques, "screeplot")


```

Again the scree plot points to 2 domains


```{r}

parallel2 <- fa.parallel(ques53,nfactors = 2, cor = "poly" )

parallel4 <- fa.parallel(ques53,nfactors = 4, cor = "poly" )

parallel6 <- fa.parallel(ques53,nfactors = 6, cor = "poly" )

parallel8 <- fa.parallel(ques53,nfactors = 8, cor = "poly" )

# documentation :
# A sad observation about parallel analysis is that it is sensitive to sample size. That is, for large data sets, the eigen values of random data are very close to 1. This will lead to different estimates of the number of factors as a function of sample size. Consider factor structure of the bfi data set (the first 25 items are meant to represent a five factor model). For samples of 200 or less, parallel analysis suggests 5 factors, but for 1000 or more, six factors and components are indicated. This is not due to an instability of the eigen values of the real data, but rather the closer approximation to 1 of the random data as n increases.
# 
# for our case, although with nfactors=1, 6 factors are suggested, when specifying nfactors =5, parallel analysis of the bfi suggests 12 factors should be extracted

```

18 factors and 6 components. 




### IFA method 

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
# mirt fits a maximum likelihood (or maximum a posteriori) factor analysis model to any mixture of dichotomous and polytomous data under the item response theory paradigm. Thus I don't have to specify "poly"

model <- c("1 dimension", "2 dimensions")

mod_graded <- mirt(ques53, 1, itemtype = "graded")
mod_graded2 <- mirt(ques53, 2, itemtype = "graded", TOL = 0.001)

anova(mod_graded, mod_graded2, verbose = T)

mod_graded3 <- mirt(ques53, 3, itemtype = "graded", TOL = 0.001)
anova(mod_graded3, mod_graded2, verbose = T)


mod_graded4 <- mirt(ques53, 4, itemtype = "graded", TOL = 0.001)
anova(mod_graded4, mod_graded3, verbose = T)
# slight decrease in AIC BIC 


anova(mod_graded4, mod_graded2, verbose = T)

anova(mod_graded3, mod_graded4, verbose = T)

# 4 models doesn't make sense, slight improvement

# after checking analysis, I figured out that reliability is very poor in the 3rd domain.  

# also I figure out that there is a huge change in ANOVA between one and two factors, but is greatly decreased from two to three factors, increase less than the removal of a single factor. 

#get no of questions that load on domain three

# mod1_v_mod2 <- rbind(M2(mod_graded), M2(mod_graded2))
# 
# rmdtbl(cbind(model, mod1_v_mod2))
```


```{r}
rmdtbl( cbind(model, anova(mod_graded, mod_graded2, verbose = F)))

```


AIC BIC and log likelihood changed dramatically when using 2 factors instead of 1. The change was not large when chaning from two to three.  

Due to the consensus across indices, the multidimensional model will be considered for further evaluation.

```{r}
# removing one by one 

### removing question 51 

# mod_52_graded3 <- mirt(ques52, 3, itemtype = "graded", TOL = 0.001)
# 
# anova(mod_graded3, mod_52_graded3, verbose = T) # p value is not significant, however, almost everything has imrpoved to a relatively high extent
# 
# stat_52_graded3 <- M2(mod_52_graded3) # didn't use type C2 since I have too many dfs 
# 
# ifit_ques52_graded <- mirt::itemfit(mod_52_graded3)
# 
# ifit_ques52_graded[ifit_ques52_graded[, 5] < 0.05, ] ## misfitting items
# 

### removing question 11

# mod_51_graded3 <- mirt(ques51, 3, itemtype = "graded", TOL = 0.001)
# 
# anova(mod_51_graded3, mod_graded3, verbose = T) # p value is not significant, however, almost everything has improved to a relatively high extent
# # after removal of questions 51 and 11, improvement is even bigger. 
# 
# ### removing 25, 40
# 
# mod_49_graded3 <- mirt(ques49, 3, itemtype = "graded", TOL = 0.001)
# 
# anova(mod_51_graded3, mod_49_graded3, verbose = T) # p value is not significant, however, almost everything has imrpoved 
# 
# anova(mod_49_graded3, mod_graded3, verbose = T) # p value is not significant, however, almost everything has imrpoved 
# 
# stat_49_graded3 <- M2(mod_49_graded3) # didn't use type C2 since I have too many dfs 
#  
# ifit_ques49_graded <- mirt::itemfit(mod_49_graded3)
# 
# ifit_ques49_graded[ifit_ques49_graded[, 5] < 0.05, ] ## misfitting items
# 

## removing 46
# 
# mod_48_graded3 <- mirt(ques48, 3, itemtype = "graded", TOL = 0.001)
# 
# anova(mod_48_graded3, mod_49_graded3, verbose = T) # p value is not significant, however, almost everything has imrpoved 
# 
# anova(mod_48_graded3, mod_graded3, verbose = T) # p value is not significant, however, almost everything has imrpoved 
# 
# stat_48_graded3 <- M2(mod_48_graded3) # didn't use type C2 since I have too many dfs 
#  
# ifit_ques48_graded <- mirt::itemfit(mod_48_graded3)
# 
# ifit_ques48_graded[ifit_ques48_graded[, 5] < 0.05, ] ## misfitting items
# 
# 
# 
# mod_48_2_graded3 <- mirt(ques48_2, 3, itemtype = "graded", TOL = 0.001)
# 
# anova(mod_48_2_graded3, mod_49_graded3, verbose = T) # p value is not significant, however, almost everything has imrpoved 
# 
# stat_48_2_graded3 <- M2(mod_48_2_graded3) # didn't use type C2 since I have too many dfs 
# 
# # remove 41
# 
# mod_48_3_graded3 <- mirt(ques48_3, 3, itemtype = "graded", TOL = 0.001)
# 
# anova(mod_48_3_graded3, mod_49_graded3, verbose = T) # p value is not significant, however, almost everything has imrpoved 
# 
# stat_48_3_graded3 <- M2(mod_48_3_graded3) # didn't use type C2 since I have too many dfs 
# 
# # End of trial. I will then examine questions based on p-value
# 
# # removing question 26
# 
# mod_48_4_graded3 <- mirt(ques48_4, 3, itemtype = "graded", TOL = 0.001)
# 
# anova(mod_48_4_graded3, mod_49_graded3, verbose = T) # p value is not significant, however, almost everything has imrpoved 
# 
# stat_48_4_graded3 <- M2(mod_48_4_graded3) # didn't use type C2 since I have too many dfs 
# 
# 
# 
# 
# # removing question 59
# 
# mod_48_5_graded3 <- mirt(ques48_5, 3, itemtype = "graded", TOL = 0.001)
# 
# anova(mod_48_5_graded3, mod_49_graded3, verbose = T) # p value is not significant, however, almost everything has imrpoved 
# 
# stat_48_5_graded3 <- M2(mod_48_5_graded3) # didn't use type C2 since I have too many dfs 

```

# questions that Dr Essam insisted to keep

19 	الحسد هو سبب إصابة ابني بالسرطان
27	أتفحص جسد أبنائي يومياً لاتأكد من عدم وجود ورم

```{r}

# try the model with 2 domians

# starting from the 53 questions

mod_53_graded2 <- mirt(ques53, 2, itemtype = "graded", TOL = 0.001)

#anova(mod_53_graded2, mod_graded2, verbose = T) # p value is not significant, however, almost everything has imrpoved 
stat_53_graded2 <- M2(mod_53_graded2) # didn't use type C2 since I have too many dfs 

rmdtbl(stat_53_graded2)

```

```{r}

# ifit_ques53_graded2 <- mirt::itemfit(mod_53_graded2)
# 
# rmv_53 <- ifit_ques53_graded2[ifit_ques53_graded2[, 5] < 0.05, ] ## misfitting items
# 
# rmdtbl(rmv_53[, c(1, 5)])

ift <- function(model){
  a <- mirt::itemfit(model)
  rmv <- a[a[, 5] < 0.05,]
  return(rmdtbl(rmv[, c(1, 5)]))
}

ift(mod_53_graded2)
```
```{r}
lo_load <- function(model, rotate){
  foo <- summary(model, rotate = rotate, verbose = F)

trial_all <- data.frame(foo$rotF) %>%
   rownames_to_column('gene') %>%
   filter_if(is.numeric, all_vars(abs(.) < 0.3)) %>%
   column_to_rownames('gene') %>%  # get only questions that have correlation less than 0.3 with all other variables. 
 rownames_to_column("Item")

return(rmdtbl(trial_all))
}

lo_load(mod_53_graded2, "oblimin")
```

choices are 51,25,40 (wording) 56 (repeated ) 26 (repeated) 

25	المنتجات الزراعية وحدها تسبب مرض السرطان

26	اكره الناس لانهم لا يقدرون مشاعري 
44	الاطباء لا يقدرون مشاعري كأم / كأب

40	يجب أن أعتمد كلياً علي المستشفي في كل ما يتعلق باحتياجات ابني

51	لا أمتنع عن تلبية كل مطالب ابني المريض

3	أتجنب التعامل مع الناس منذ مرض ابني
56	أخشي التعامل مع الناس بعد إصابة ابني بالمرض

### I will remove 26 - 56 - 51

```{r}

ques50 <- questions %>% 
  dplyr::select(-c(7, 12, 23, 30, 9, 47, 16, 49, 26, 56, 51))

mod_50_graded2 <- mirt(ques50, 2, itemtype = "graded", TOL = 0.001)

rmdtbl(anova(mod_50_graded2, mod_53_graded2, verbose = T)) # p value is not significant, however, almost everything has imrpoved 
```


```{r}
stat_50_graded2 <- M2(mod_50_graded2) # didn't use type C2 since I have too many dfs 

rmdtbl(bind_rows(stat_50_graded2, stat_53_graded2))
# possible a deterioration, but we need to remove the question
```

```{r}
lo_load(mod_50_graded2, "oblimin")

```

```{r}
ift(mod_50_graded2)

```

nothing wrong with wording. P value is slightly significant. I will start with loadings

# Remove 25

```{r}

ques49 <- questions %>% 
  dplyr::select(-c(7, 12, 23, 30, 9, 47, 16, 49, 26, 56, 51, 25))

mod_49_graded2 <- mirt(ques49, 2, itemtype = "graded", TOL = 0.001)

rmdtbl(anova(mod_49_graded2, mod_50_graded2, verbose = T)) # p value is not significant, however, almost everything has imrpoved 
```


```{r}
stat_49_graded2 <- M2(mod_49_graded2) # didn't use type C2 since I have too many dfs 

rmdtbl(bind_rows(stat_49_graded2, stat_50_graded2))
# possible a deterioration, but we need to remove the question
```

```{r}
lo_load(mod_49_graded2, "oblimin")

```

```{r}
ift(mod_49_graded2)

```
can't remove 19 

# remove 11 

```{r}

ques48 <- questions %>% 
  dplyr::select(-c(7, 12, 23, 30, 9, 47, 16, 49, 26, 56, 51, 25, 11))

mod_48_graded2 <- mirt(ques48, 2, itemtype = "graded", TOL = 0.001)

rmdtbl(anova(mod_48_graded2, mod_49_graded2, verbose = T)) # p value is not significant, however, almost everything has imrpoved 
```


```{r}
stat_48_graded2 <- M2(mod_48_graded2) # didn't use type C2 since I have too many dfs 

rmdtbl(bind_rows(stat_48_graded2, stat_49_graded2))
# possible a deterioration, but we need to remove the question
```

```{r}
lo_load(mod_48_graded2, "oblimin")
lo_load(mod_48_graded2, "varimax")

```

item 15 has been added to oblimin but not to varimax. loading is 0.299
wording is ok .. I will keep it

```{r}
ift(mod_48_graded2)

```


# remove 40


```{r}
ques47 <- questions %>% 
  dplyr::select(-c(7, 12, 23, 30, 9, 47, 16, 49, 26, 56, 51, 25, 11, 40))

mod_47_graded2 <- mirt(ques47, 2, itemtype = "graded", TOL = 0.001)

rmdtbl(anova(mod_47_graded2, mod_49_graded2, verbose = T)) # p value is not significant, however, almost everything has imrpoved 
```


```{r}
stat_47_graded2 <- M2(mod_47_graded2) # didn't use type C2 since I have too many dfs 

rmdtbl(bind_rows(stat_47_graded2, stat_48_graded2))
# possible a deterioration, but we need to remove the question
```

```{r}
lo_load(mod_47_graded2, "oblimin")
lo_load(mod_47_graded2, "varimax")

```


```{r}
ift(mod_47_graded2)

```
14	أشعر بالذنب لتقصيري في حماية ابني من المرض
22	زوجي ( زوجتي ) يعتبرني المسئول الوحيد عن مرض ابني
35	أكره زوجي ( زوجتي ) لمسئوليته عن مرض ابني

for question 22, 83% responsed by لا ترد 

for question 35, 87.6% responded by لا ترد 

will remove 22 since p value is more significant. In addition, "only" I think is somehow misleading.

# remove 22


```{r}
ques46 <- questions %>% 
  dplyr::select(-c(7, 12, 23, 30, 9, 47, 16, 49, 26, 56, 51, 25, 11, 40, 22))

mod_46_graded2 <- mirt(ques46, 2, itemtype = "graded", TOL = 0.001)

rmdtbl(anova(mod_46_graded2, mod_47_graded2, verbose = T)) # p value is not significant, however, almost everything has imrpoved 
```


```{r}
stat_46_graded2 <- M2(mod_46_graded2) # didn't use type C2 since I have too many dfs 

rmdtbl(bind_rows(stat_46_graded2, stat_47_graded2))
# possible a deterioration, but we need to remove the question
```

```{r}
lo_load(mod_46_graded2, "oblimin")
lo_load(mod_46_graded2, "varimax")

```


```{r}
ift(mod_46_graded2)

```


# remove 2

2	أشعر أن من حقي أن يحزن الناس لحزني علي  أبني 

```{r}
ques45 <- questions %>% 
  dplyr::select(-c(7, 12, 23, 30, 9, 47, 16, 49, 26, 56, 51, 25, 11, 40, 22, 2))

mod_45_graded2 <- mirt(ques45, 2, itemtype = "graded", TOL = 0.001)

rmdtbl(anova(mod_45_graded2, mod_46_graded2, verbose = T)) # p value is not significant, however, almost everything has imrpoved 
```


```{r}
stat_45_graded2 <- M2(mod_45_graded2) # didn't use type C2 since I have too many dfs 

rmdtbl(bind_rows(stat_45_graded2, stat_46_graded2))
# possible a deterioration, but we need to remove the question
```

```{r}
lo_load(mod_45_graded2, "oblimin")
lo_load(mod_45_graded2, "varimax")

```


```{r}
ift(mod_45_graded2)

```

47	أخاف أن يصاب ابني بمرض آخر نتيجة العلاج
53	أخاف من المضاعفات المترتبة عن العلاج 

 -------- 47 was previously removed

29	أشعر بالوصمة والعار لمرض ابني بالسرطان   must keep

47 I think is more clear to the irrational thoughts ... for 53 every patient will experience side effects

# exchange 47 with 53


```{r}
ques45_2 <- questions %>% 
  dplyr::select(-c(7, 12, 23, 30, 9, 53, 16, 49, 26, 56, 51, 25, 11, 40, 22, 2))

mod_45_2_graded2 <- mirt(ques45_2, 2, itemtype = "graded", TOL = 0.001)

rmdtbl(anova(mod_45_2_graded2, mod_45_graded2, verbose = T)) # p value is not significant, however, almost everything has imrpoved 
```


```{r}
stat_45_2_graded2 <- M2(mod_45_2_graded2) # didn't use type C2 since I have too many dfs 

rmdtbl(bind_rows(stat_45_2_graded2, stat_45_graded2))
```


```{r}
lo_load(mod_45_2_graded2, "oblimin")
lo_load(mod_45_2_graded2, "varimax")

```


```{r}
ift(mod_45_2_graded2)

```

36	أتشاجر مع الممرضة إذا تأخرت عن تلبية ندائي 

I think yes, the questions is misleading.

# remove 36

```{r}
ques44 <- questions %>% 
  dplyr::select(-c(7, 12, 23, 30, 9, 53, 16, 49, 26, 56, 51, 25, 11, 40, 22, 2, 36))

mod_44_graded2 <- mirt(ques44, 2, itemtype = "graded", TOL = 0.001)

rmdtbl(anova(mod_44_graded2, mod_45_2_graded2, verbose = T)) # p value is not significant, however, almost everything has imrpoved 
```


```{r}
stat_44_graded2 <- M2(mod_44_graded2) # didn't use type C2 since I have too many dfs 

rmdtbl(bind_rows(stat_44_graded2, stat_45_2_graded2))
```

```{r}
lo_load(mod_44_graded2, "oblimin")
lo_load(mod_44_graded2, "varimax")

```


```{r}
ift(mod_44_graded2)

```
14	أشعر بالذنب لتقصيري في حماية ابني من المرض 

I think I will keep this question

Question 1

1	أعتقد بأن الشفاء من الله ولا أهمية للطب أو العلاج  

I think can be misinterpreted as something positive. (كل شيء بمشيئة الله)

After removal, the model deteriorates. I think I will keep it. 

In addition, I think this item is unique. 

In addition, I have tried gpcm, but the improvement was minimal. Also a large number of questions were loading very poorly

# Final decision 

keep 44 questions

remove : (7, 12, 23, 30, 9, 53, 16, 49, 26, 56, 51, 25, 11, 40, 22, 2, 36)


```{r}
lo_35 <- function(model, rotate){
  foo <- summary(model, rotate = rotate, verbose = F)

trial_all <- data.frame(foo$rotF) %>%
   rownames_to_column('gene') %>%
   filter_if(is.numeric, all_vars(abs(.) < 0.35)) %>%
   column_to_rownames('gene') %>%  # get only questions that have correlation less than 0.3 with all other variables. 
 rownames_to_column("Item")

return(rmdtbl(trial_all))
}


lo_35(mod_44_graded2, "oblimin")

lo_35(mod_44_graded2, "varimax")

foo <- summary(mod_44_graded2, rotate = "oblimin", verbose = F)

fo2 <- data.frame(foo$rotF)

f2_better <- fo2[abs(fo2$F2) > abs(fo2$F1),]

sort(f2_better$F2)

f1_better <- fo2[abs(fo2$F2) < abs(fo2$F1),]

sort(f1_better$F1)

check <- fo2[abs(fo2$F1) - abs(fo2$F2) < 0.05 & abs(fo2$F1) - abs(fo2$F2) > -0.05 ,]

foo_v <- summary(mod_44_graded2, rotate = "varimax")

fo2_v <- data.frame(foo_v$rotF)

f2_better_v <- fo2_v[abs(fo2_v$F2) > abs(fo2_v$F1),]

sort(f2_better$F2)

f1_better_v <- fo2_v[abs(fo2_v$F2) < abs(fo2_v$F1),]

sort(f1_better$F1)

check_v <- fo2_v[abs(fo2_v$F1) - abs(fo2_v$F2) < 0.05 & abs(fo2_v$F1) - abs(fo2_v$F2) > -0.05 ,]

```

4/14/15/52 These are the problematic questions.

4	مرض ابني عقاب من الله علي ما ارتكبته من ذنوب

can't remove this questions

52	الحل الوحيد لعلاج السرطان هو استئصال الورم 

what if the patient has leukemia ?

# remove 52 
  
```{r}
ques43 <- questions %>% 
  dplyr::select(-c(7, 12, 23, 30, 9, 53, 16, 49, 26, 56, 51, 25, 11, 40, 22, 2, 36, 52))

mod_43_graded2 <- mirt(ques43, 2, itemtype = "graded", TOL = 0.001, SE = T)

rmdtbl(anova(mod_43_graded2, mod_44_graded2, verbose = T)) # p value is not significant, however, almost everything has imrpoved 

extract.mirt(mod_43_graded2, 'DIC')
extract.mirt(mod_44_graded2, 'DIC')


```

AIC, BIC, and the LR test are based on maximum likelihood estimation of the model parameters and are not suitable when Bayesian estimation has been used (Lin & Dayton, 1997). In the context of MCMC estimation, models can be compared via the deviance information criterion (DIC; Spiegelhalter, Best, Carlin, & van der Linde, 2002). Like the AIC and BIC, this index assesses goodness of fit and includes a penalty for complexity, but DIC differs by employing a Bayesian (rather than likelihood-based) measure of fit known as the posterior mean deviance. As with other information criteria, the better model is the one with the lower DIC.


```{r}
stat_43_graded2 <- M2(mod_43_graded2) # didn't use type C2 since I have too many dfs 

rmdtbl(bind_rows(stat_43_graded2, stat_44_graded2))

```

Adequate model fit is indicated by RMSEA ≤0.05/ (k-1) where K is the number of response categories (Maydeu-Olivares & Joe, 2014). Here there is a little deviation from this cut-off. Maydeu-Olivares (2013) comments, however, that more work is needed in the area of IRT fit assessment, particularly with regard to the performance of M2 and other fit statistics in applied settings rather than simulation studies.


```{r}
lo_load(mod_43_graded2, "oblimin")
lo_load(mod_43_graded2, "varimax")

```


```{r}
ift(mod_43_graded2)

```


```{r}

fs <- coef(mod_43_graded2, simplify = T, as.data.frame = F)

fes <-rownames_to_column(data.frame(fs$items))  

fes %>% 
  arrange(a1) %>% 
  head()

#items 59 and 60 again are the easiest to answer with 1 

fes %>% 
  arrange(desc(a1)) %>% 
  head()

# items 34 and 31 are the easiest to answer with 3 


fes %>% 
  arrange(desc(a2)) %>% 
  head()

#items 39 and 35 are the easiest to answer with 3

fes %>% 
  arrange(a2) %>% 
  head()

# items 59 and 60 again are easiest to answer with 1 

# no estimates with extreme values 


```

```{r}
conf <- coef(cmod, simplify = T, as.data.frame = F)

conf2 <-rownames_to_column(data.frame(conf$items))  

sort(conf2$a1)
sort(conf2$a2)

```

for the first domain, five items are out of the range of Good” discrimination parameters
for the second domain, only a single item is out of the range of the Good” discrimination parameters (de
Ayala, 2009)

No items are too steep, but the bad items are too flat. 



### Domain 2 questions

```{r}
foo <- summary(mod_43_graded2, rotate = "oblimin", verbose = F)

fo2 <- data.frame(foo$rotF)

f2_better <- data.frame(fo2[abs(fo2$F2) > abs(fo2$F1),])

f2_better <- rownames_to_column(f2_better)

names(f2_better) <- c("Item", "Domain_1_loadings", "Domain_2_loadings")

#sort(f2_better$Domain_2_loadings)

rmdtbl(f2_better)
```

### Domain 1 questions

```{r}
f1_better <- data.frame(fo2[abs(fo2$F2) < abs(fo2$F1),])

f1_better <- rownames_to_column(f1_better)

names(f1_better) <- c("Item", "Domain_1_loadings", "Domain_2_loadings")

rmdtbl(f1_better)


```

### Items to decide

```{r}
#sort(f1_better$F1)

check <- fo2[abs(fo2$F1) - abs(fo2$F2) < 0.05 & abs(fo2$F1) - abs(fo2$F2) > -0.05 ,]

check <- rownames_to_column(check)

rmdtbl(check)

# foo_v <- summary(mod_43_graded2, rotate = "varimax")
# 
# fo2_v <- data.frame(foo_v$rotF)
# 
# f2_better_v <- fo2_v[abs(fo2_v$F2) > abs(fo2_v$F1),]
# 
# sort(f2_better$F2)
# 
# f1_better_v <- fo2_v[abs(fo2_v$F2) < abs(fo2_v$F1),]
# 
# sort(f1_better$F1)
# 
# check_v <- fo2_v[abs(fo2_v$F1) - abs(fo2_v$F2) < 0.05 & abs(fo2_v$F1) - abs(fo2_v$F2) > -0.05 ,]

```

### confirmatory model

```{r}

# match(f1_better$Item, names(ques43)) # get the column number from the name

# match(f2_better$Item, names(ques43))  # get the column number from the name

model <- mirt.model('F1 = 1, 2,  4,  5,  9, 10, 11, 13, 15, 17, 21, 26, 28, 30, 32, 33, 34, 41, 42, 43
 F2 = 3,  6,  7,  8, 12, 14, 16, 18, 19, 20, 22, 23, 24, 25, 27, 29, 31, 35, 36, 37, 38, 39, 40
                    COV = F1*F2')

cmod <- mirt(ques43, model, SE=T)   # phil in his example didn't use MHRM, however, the book modern psychometric has used them.  
coef(cmod)
summary(cmod)

asticmod <- summary(cmod, verbose = FALSE)
round(asticmod$fcor, 3)

M2(cmod)

# results are good 

for(i in 1:ncol(ques43)){
  print(itemplot(cmod, i, drop.zeros = TRUE)) #drop zeros from plot, lowers dimensions
}

csummary <- summary(cmod, rotate = "oblimin", verbose = F)

cload <- data.frame(csummary$rotF)
# max(cload)
# sort(cload$F1)
# sort(cload$F2)


```

```{r}
person_par <- data.frame(fscores(mod_43_graded2, method = 'EAPsum'))

# Local independence means that given a person parameter, item responses become
# independent. This assumption is difficult to check and often omitted in practice.

# (ii <- itemfit(mod_43_graded2))
# p.adjust(ii$p.S_X2, 'fdr')

#without adjustment, nothing is significant. I won't apply adjustment. 

# SX2_tabs <- itemfit(mod_43_graded2, S_X2.tables = TRUE)
# SX2_tabs$O.org[[1]]
# SX2_tabs$O[[1]]
# SX2_tabs$E[[1]]

## these 4 lines above are in example 2. I don't understand how to use them  


#item residual covaration
# help('residuals-method')
# residuals(mod_43_graded2)
# residuals(mod_43_graded2, tables=TRUE)

# still I can't understand

# I think LR test takes into consideration the residuals automatically. Thus, I won't report them here. 


```



# comparing means for person parameter

```{r}

summary(aov(person_par$Theta.1 ~ SurveyData$income_category_comb, data = SurveyData))

summary(aov(person_par$Theta.1 ~ SurveyData$Number_of_family_members, data = SurveyData))

summary(aov(person_par$Theta.1 ~ SurveyData$parent_education
, data = SurveyData))

summary(aov(person_par$Theta.1 ~ SurveyData$sex, data = SurveyData))


summary(aov(person_par$Theta.2 ~ SurveyData$income_category_comb, data = SurveyData))

summary(aov(person_par$Theta.2 ~ SurveyData$Number_of_family_members, data = SurveyData))

summary(aov(person_par$Theta.2 ~ SurveyData$parent_education
, data = SurveyData))

summary(aov(person_par$Theta.2 ~ SurveyData$sex, data = SurveyData))

```

# MANOVA

```{r}


mnv_tbl <- function(col){
  
  fit_sex <- tidy(manova(as.matrix(person_par) ~ col ), test = "Hotelling-Lawley")

fit_sex <- fit_sex[1, c(1,2,3,4,7)]

colnames(fit_sex) <- c("Factor", "df", "Hotelling-Lawley trace", "F", "p.value")

return(fit_sex)  
}

mnv_tbl(SurveyData$income_category_comb)

mnv_tbl(SurveyData$sex)

mnv_tbl(SurveyData$parent_education)

```


### Reliability 

```{r}

summary(mod_43_graded2)

Mod44.REL <- fscores(mod_44_graded2, method= "EAP", returnER = T)

Mod43.REL <- bind_rows(fscores(mod_43_graded2, method= "EAP", returnER = T))

rel_compare <- bind_rows(Mod44.REL, Mod43.REL)

```


The ~~first~~ subscale has a reliability of `r round(Mod43.REL$F1, 3)`, and the ~~second~~ subscale had a reliability of `r round(Mod43.REL$F1, 3)`.


# Parameter estimates

https://groups.google.com/forum/#!searchin/mirt-package/intercept$20and$20difficulty|sort:date/mirt-package/AWf_JdTfCmo/09Qm9892BQAJ  

according to this link, I will not use difficulty parameter. Since Phil(MIRT developer) said "How is) multidimensional difficulty more interpretable than an intercept? Difficulty generally reflects the location where the response function inflects. In multidimensional models, this occur as vector or hyper-vector locations, so as humans we really can't think in those terms too well."

```{r}

f1_better
f2_better

model<-mirt.model("
                  F1=1-9
                  F2 =10-61
                  INT=17-25
                  ATH=26-30
                  COV=INFO*PRESS*INT*ATH")

par_est <- coef(mod_43_graded2, printSE=T, as.data.frame = F, rotate = 'oblimin') 

```

```{r}
# remove 15 ( wan't beneficial)

# ques42 <- questions %>% 
#   dplyr::select(-c(7, 12, 23, 30, 9, 53, 16, 49, 26, 56, 51, 25, 11, 40, 22, 2, 36, 52, 15))
# 
# mod_42_graded2 <- mirt(ques42, 2, itemtype = "graded", TOL = 0.001)
# 
# rmdtbl(anova(mod_42_graded2, mod_43_graded2, verbose = T)) 

```


```{r}
# stat_42_graded2 <- M2(mod_42_graded2) # didn't use type C2 since I have too many dfs 
# 
# rmdtbl(bind_rows(stat_42_graded2, stat_43_graded2))
```

```{r}
# lo_load(mod_42_graded2, "oblimin")
# lo_load(mod_42_graded2, "varimax")

```


```{r}
# ift(mod_42_graded2)


```



```{r}
# gpcm 

# mod_44_graded2_gpcm <- mirt(ques44, 2, itemtype = "gpcm", TOL = 0.001)
# rmdtbl(anova(mod_44_graded2_gpcm, mod_44_graded2, verbose = T)) # p value is not significant, however, almost everything has imrpoved 
```


```{r}
# stat_44_graded2_gpcm <- M2(mod_44_graded2_gpcm) # didn't use type C2 since I have too many dfs 
# 
# rmdtbl(bind_rows(stat_44_graded2_gpcm, stat_44_graded2))
```

```{r}
# lo_load(mod_44_graded2_gpcm, "oblimin")
# lo_load(mod_44_graded2_gpcm, "varimax")

```


```{r}
#ift(mod_44_graded2_gpcm)

```



```{r}

# removing question 1
# ques43 <- questions %>% 
#   dplyr::select(-c(7, 12, 23, 30, 9, 53, 16, 49, 26, 56, 51, 25, 11, 40, 22, 2, 36, 1))
# 
# mod_43_graded2 <- mirt(ques43, 2, itemtype = "graded", TOL = 0.001)
# 
# rmdtbl(anova(mod_43_graded2, mod_44_graded2, verbose = T)) # p value is not significant, however, almost everything has imrpoved 
```


```{r}
# stat_43_graded2 <- M2(mod_43_graded2) # didn't use type C2 since I have too many dfs 
# 
# rmdtbl(bind_rows(stat_43_graded2, stat_44_graded2))
```

```{r}
# lo_load(mod_43_graded2, "oblimin")
# lo_load(mod_43_graded2, "varimax")

```


```{r}
# ift(mod_43_graded2)

```



```{r}
 # foo <- summary(mod_51_graded3, rotate = "oblimin")
 # 
 # trial_all <- data.frame(foo$rotF) %>%
 #   rownames_to_column('gene') %>%
 #   filter_if(is.numeric, all_vars(abs(.) < 0.3)) %>%
 #   column_to_rownames('gene')   # get only questions that have correlation less than 0.3 with all other variables. 
 # 
 # foo_v <- summary(mod_51_graded3, rotate = "varimax")
 # 
 # trial_all_v <- data.frame(foo_v$rotF) %>%
 #   rownames_to_column('gene') %>%
 #   filter_if(is.numeric, all_vars(abs(.) < 0.3)) %>%
 #   column_to_rownames('gene')   # get only questions that have correlation less than 0.3 with all other variables. 

```

Thus it seems that although we have 10 questions that load in domain 3, reliability is much better in two domains

```{r}
trial_all <- data.frame(foo$rotF) %>%
  rownames_to_column('gene') %>%
  filter_if(is.numeric, all_vars(abs(.) < 0.3)) %>%
  column_to_rownames('gene')   # get only questions that have correlation less than 0.3 with all other variables.

foo_v <- summary(mod_49_graded3, rotate = "varimax")

trial_all_v <- data.frame(foo_v$rotF) %>%
  rownames_to_column('gene') %>%
  filter_if(is.numeric, all_vars(abs(.) < 0.3)) %>%
  column_to_rownames('gene')   # get only questions that have correlation less than 0.3 with all other variables.

foo <- summary(mod_49_graded2, rotate = "oblimin")

trial_all <- data.frame(foo$rotF) %>%
  rownames_to_column('gene') %>%
  filter_if(is.numeric, all_vars(abs(.) < 0.3)) %>%
  column_to_rownames('gene')   # get only questions that have correlation less than 0.3 with all other variables. 

foo_v <- summary(mod_49_graded2, rotate = "varimax")

trial_all_v <- data.frame(foo_v$rotF) %>%
  rownames_to_column('gene') %>%
  filter_if(is.numeric, all_vars(abs(.) < 0.3)) %>%
  column_to_rownames('gene')   # get only questions that have correlation less than 0.3 with all other variables. 


```

Results here indicate that only 3 questions are to be removed from the model based on the two factor design 

# try the idea on the model based only on the removal from Dr. Essam

```{r}

foo <- summary(mod_52_graded3, rotate = "oblimin")

trial_all <- data.frame(foo$rotF) %>%
  rownames_to_column('gene') %>%
  filter_if(is.numeric, all_vars(abs(.) < 0.3)) %>%
  column_to_rownames('gene')   # get only questions that have correlation less than 0.3 with all other variables. 

foo_v <- summary(mod_52_graded3, rotate = "varimax")

trial_all_v <- data.frame(foo_v$rotF) %>%
  rownames_to_column('gene') %>%
  filter_if(is.numeric, all_vars(abs(.) < 0.3)) %>%
  column_to_rownames('gene')   # get only questions that have correlation less than 0.3 with all other variables. 

```



##### I have chosen this method since Dave suggested using M2 

I have chosen graded response model since it appeared from Dr Essam talk that maybe some people are afraid to answer, thus they may choose "sometime crosses my mind" while in fact they mean this thing. So I thought graded response model would be better at describing the data. 

```{r}

#mod_gpcm <- mirt(ques53, 2, itemtype = "gpcm")
# mod_pc <- mirt(ques53, 2, itemtype = "PC3PL") Partially compensatory models can only be estimated within a confirmatory model
#mod_ggum <- mirt(ques53, 2, itemtype = "ggum")  not used. check MIRT word document 
#mod_seq <- mirt(ques53, 2, itemtype = "sequential", TOL = 0.001) # not used, check MIRT word document
#mod_spline <- mirt(ques53, 2, itemtype = "spline") # no restrictions on the model regarding maths, over sophisticated 

# stat_graded2 <- M2(mod_graded2) # didn't use type C2 since I have too many dfs 
# stat_graded3 <- M2(mod_graded3) # didn't use type C2 since I have too many dfs 

stat_51_graded3 <- M2(mod_51_graded3) # didn't use type C2 since I have too many dfs 

#stat_gpcm <- M2(mod_gpcm)

```

statistics reveal here that 3 factors is better than 2 factors 

This even better after removing questions 51 and 11

as per the modern psychometrics book 
For the CFI, we can use the same 0.95 fit
cutoff as in CFA/SEM. For the RMSEA, the CFA/SEM cutoff was 0.05 for a good
fitting model. In IRT, it is suggested to use 0.05/k (with k being the number of
categories per item) as fit cutoff.


https://groups.google.com/forum/#!topic/mirt-package/gy63tCz2W98   ( same as SEM, 0.9 for CFI and 0.05 for RMSEA)

Do you have a large sample? If so could be that small differences in expected vs observed response freqs are causing significant p values even though magnitude of misfit is not that big per empirical item plots (and S-X2 RMSEA) https://groups.google.com/forum/#!searchin/mirt-package/M2|sort:date/mirt-package/ZWFimGtUEsI/MXx6pXoaBgAJ 

 
### item fit 

##### graded 

```{r}

ifit_ques51_graded <- mirt::itemfit(mod_51_graded3)

ifit_ques51_graded[ifit_ques51_graded[, 5] < 0.05, ] ## misfitting items
# 
# ifit_ques53_graded <- mirt::itemfit(mod_graded3)
# 
# ifit_ques53_graded[ifit_ques53_graded[, 5] < 0.05, ] ## misfitting items

```

A single questions is slightly missfitting, and I would like to keep it. 


```{r}
##### gpcm 

# ifit_ques53_gpcm <- mirt::itemfit(mod_gpcm)
# 
# ifit_ques53_gpcm[ifit_ques53[, 5] < 0.05, ] ## misfitting items

```

Here is it recommended by factor loadings that we should remove 5 questions. (25,27,40,41,46)

On examining the questions, it appears that some questions should in fact be removed as 25 and 40.. 

On chaning the rotation to varimax, three questions are consistent (25,27,40)

I will remove 25 and 40, since there phrasing is strange and I will recheck.

# information plot

```{R}
itemplot(mod_51_graded3, 3, rot = list(xaxis = -70, yaxis = 50, zaxis = 10, shiny = T))

itemplot(mod_43_graded2, 1, shiny = T )

# information plots to see where each item is highly informative across the latent trait
for(i in 1:ncol(ques43)){
  print(itemplot(cmod, i, drop.zeros = TRUE, type = "info")) #drop zeros from plot, lowers dimensions
}

#names(ques43)[24]

# question 35 is a highly informative question at a latent trait of 1.2 

names(ques43)[27]

# question 39 is a highly informative question at a latent trait of 1.2 


names(ques43)[20]

# question 31 is a highly informative question at a latent trait of 1.2 


names(ques43)[19]

# question 29 is a highly informative question at a latent trait of 1.2 


```

# category response plots with contours for the exploratory model

```{r}


# category response plots with contours to see the probability of responding in a certain way
# for(i in 1:ncol(ques43)){
#   print(itemplot(mod_43_graded2, i,  type = "tracecontour")) 
# }


#Figure 4.1 - MGRM category response surfaces

p0.grm <- function(a1,a2,b11,b12, no) {
  prob <- matrix(0,length(theta1),length(theta2))
  for (i in 1:length(theta1)) {
    for (j in 1:length(theta2)) {
      z <- a1*(theta1[i]-b11)+a2*(theta2[j]-b12)
      p0star <- rep(1, ncol(ques43))
      p1star <- exp(z)/(1+exp(z))
      
      prob[i,j] <- p0star[no]-p1star[no]
    }
  }
  prob
}


p1.grm <- function(a1,a2,b11,b12,b21,b22, no) {
  prob <- matrix(0,length(theta1),length(theta2))
  for (i in 1:length(theta1)) {
    for (j in 1:length(theta2)) {
      z0 <- a1*(theta1[i]-b11)+a2*(theta2[j]-b12)
      z1 <- a1*(theta1[i]-b21)+a2*(theta2[j]-b22)
      p0star <- exp(z0)/(1+exp(z0))
      p1star <- exp(z1)/(1+exp(z1))
      prob[i,j] <- p0star[no]-p1star[no]
    }
  }
  prob
}

p2.grm <- function(a1,a2,b21,b22, no) {
  prob <- matrix(0,length(theta1),length(theta2))
  for (i in 1:length(theta1)) {
    for (j in 1:length(theta2)) {
      z1 <- a1*(theta1[i]-b21)+a2*(theta2[j]-b22)
      p1star <- exp(z1)/(1+exp(z1))
      p2star <- rep(0, ncol(ques43))
      prob[i,j] <- p1star[no]-p2star[no]
    }
  }
  prob
}

kgl <- coef(mod_43_graded2, simplify = T)
kgl <- data.frame(kgl$items)


#arbitrary parameters
theta1 <- seq(-4,4,.25)
theta2 <- seq(-4,4,.25)

kgl$a2[kgl$a2 == 0] <- 0.01

a1 <- kgl$a1
a2 <- kgl$a2
b11 <- -kgl$d1/kgl$a1
b12 <- -kgl$d1/kgl$a2
b21 <- -kgl$d2/kgl$a1
b22 <- -kgl$d2/kgl$a2


x <- theta1
y <- theta2


for (i in 1:ncol(ques43)){
  contour(x,y,p0.grm(a1,a2,b11,b12,i),las=1,labcex=1,xlab=expression(theta[1]),cex.lab=1.5,ylab=expression(theta[2]))
}

for (i in 1:ncol(ques43)){
  contour(x,y, p1.grm(a1,a2,b11,b12,b21,b22,i),las=1,labcex=1,xlab=expression(theta[1]),cex.lab=1.5,ylab=expression(theta[2]))
}

for (i in 1:ncol(ques43)){
  contour(x,y,p2.grm(a1,a2,b21,b22,i),las=1,labcex=1,xlab=expression(theta[1]),cex.lab=1.5,ylab=expression(theta[2]))
}


```

# compensatory plots

```{r}
for (i in 1:ncol(ques43)){ 
slice <- -2
yslice <- which(y==slice)
#windows(4,4)
plot(x,p0.grm(a1,a2,b11,b12, i)[,yslice],type='l',ylim=c(0,1.1),las=1,xlim=c(-4,4),ylab='P(x = k)',xlab=expression(theta[1]),lwd=2)
lines(x,p1.grm(a1,a2,b11,b12,b21,b22, i)[,yslice],lwd=2,lty=2)
lines(x,p2.grm(a1,a2,b21,b22, i)[,yslice],lwd=2,lty=3)
key <- as.expression(bquote(paste(theta[1],' | ',theta[2])==.(slice)))
legend("top",legend=key,cex=1.5,bty='n')

}
```

Examining these slices will help us make sense of the unintuitive surface plot. For instance, the top panel shows that when θ2 is high, response category k = 1 (the solid line) is only the most likely choice when the respondent is low (<–1.5) on the θ1 scale. Similarly, the bottom panel shows that when examinees are low on θ2, then response category 3 is only likely when θ1 is very high (>3.0). In terms of the example social situation enjoyment item from Chapter 4, these conditional cross-sections tell us that regardless of anxiety (θ1) level, an examinee is unlikely to select “Always” (i.e., k = 0) if she is highly introverted (e.g., θ2 = 2) or “Never” (i.e., k = 2) if she is highly extroverted (e.g., θ2 = –2). 


```{r}
for (i in 1:ncol(ques43)){ 
slice <- 0
yslice <- which(y==slice)
#windows(4,4)
plot(x,p0.grm(a1,a2,b11,b12, i)[,yslice],type='l',ylim=c(0,1.1),las=1,xlim=c(-4,4),ylab='P(x = k)',xlab=expression(theta[1]),lwd=2)
lines(x,p1.grm(a1,a2,b11,b12,b21,b22, i)[,yslice],lwd=2,lty=2)
lines(x,p2.grm(a1,a2,b21,b22, i)[,yslice],lwd=2,lty=3)
key <- as.expression(bquote(paste(theta[1],' | ',theta[2])==.(slice)))
legend("top",legend=key,cex=1.5,bty='n')

}
```

```{r}

for (i in 1:ncol(ques43)){ 
slice <- 2
yslice <- which(y==slice)
#windows(4,4)
plot(x,p0.grm(a1,a2,b11,b12, i)[,yslice],type='l',ylim=c(0,1.1),las=1,xlim=c(-4,4),ylab='P(x = k)',xlab=expression(theta[1]),lwd=2)
lines(x,p1.grm(a1,a2,b11,b12,b21,b22, i)[,yslice],lwd=2,lty=2)
lines(x,p2.grm(a1,a2,b21,b22, i)[,yslice],lwd=2,lty=3)
key <- as.expression(bquote(paste(theta[1],' | ',theta[2])==.(slice)))
legend("top",legend=key,cex=1.5,bty='n')

}

```

# modify theta 1 

```{r}

for (i in 1:ncol(ques43)){ 
slice <- -2
xslice <- which(x==slice)
#windows(4,4)
plot(x,p0.grm(a1,a2,b11,b12, i)[xslice,],type='l',ylim=c(0,1.1),las=1,xlim=c(-4,4),ylab='P(x = k)',xlab=expression(theta[2]),lwd=2)
lines(x,p1.grm(a1,a2,b11,b12,b21,b22, i)[xslice,],lwd=2,lty=2)
lines(x,p2.grm(a1,a2,b21,b22, i)[xslice,],lwd=2,lty=3)
key <- as.expression(bquote(paste(theta[2],' | ',theta[1])==.(slice)))
legend("top",legend=key,cex=1.5,bty='n')

}
```

```{r}
for (i in 1:ncol(ques43)){ 
slice <- 0
xslice <- which(x==slice)
#windows(4,4)
plot(x,p0.grm(a1,a2,b11,b12, i)[xslice,],type='l',ylim=c(0,1.1),las=1,xlim=c(-4,4),ylab='P(x = k)',xlab=expression(theta[2]),lwd=2)
lines(x,p1.grm(a1,a2,b11,b12,b21,b22, i)[xslice,],lwd=2,lty=2)
lines(x,p2.grm(a1,a2,b21,b22, i)[xslice,],lwd=2,lty=3)
key <- as.expression(bquote(paste(theta[2],' | ',theta[1])==.(slice)))
legend("top",legend=key,cex=1.5,bty='n')

}
```

```{r}

for (i in 1:ncol(ques43)){ 
slice <- 2
xslice <- which(x==slice)
#windows(4,4)
plot(x,p0.grm(a1,a2,b11,b12, i)[xslice,],type='l',ylim=c(0,1.1),las=1,xlim=c(-4,4),ylab='P(x = k)',xlab=expression(theta[2]),lwd=2)
lines(x,p1.grm(a1,a2,b11,b12,b21,b22, i)[xslice,],lwd=2,lty=2)
lines(x,p2.grm(a1,a2,b21,b22, i)[xslice,],lwd=2,lty=3)
key <- as.expression(bquote(paste(theta[2],' | ',theta[1])==.(slice)))
legend("top",legend=key,cex=1.5,bty='n')

}

```


# expected score surface

```{r}

for (i in 1:ncol(ques43)){ 

score <- p0.grm(a1,a2,b11,b12, i)*0+p1.grm(a1,a2,b11,b12,b21,b22, i)*1+p2.grm(a1,a2,b21,b22, i)*2
# persp(x,y,score,phi=18,theta=20,d=2,zlim=c(0,2),xlab="\ntheta1",ylab="\ntheta2",zlab="\nExpected Score",ticktype='detailed',cex.lab=1.3)
contour(x,y,score,levels=c(0,.5,1,1.5),las=1,labcex=1,xlab=expression(theta[1]),cex.lab=1.5,ylab=expression(theta[2]),lwd=seq(0,5,length=4))
}

```

One descriptive tool that is unique to polytomous models is the expected item response score, meaning the observed response that we would expect to see given the respondent’s location in the θ-space. In dichotomous models, of course, there are only two scores, so the idea of an expected score is redundant. But when there are multiple scores (i.e., k = 0, 1, or 2), then it may be useful to examine the expected score in addition to the probability of selecting a certain response. 

Notice that the z-axis in the panel on the left now represents the expected score (which ranges from 0 to 2) for our three category items, rather than the response probability (which ranges from 0 to 1). In terms of the social situations example, where θ1 is anxiety and θ2 is introversion, these plots reveal that individuals who are high in anxiety (θ1 = 2) but low in introversion (θ2 = –2), for example, are expected to select response category 1 (Sometimes; i.e., provide a score of 1), while those who have average levels of anxiety (θ1 = 0) but are highly extroverted (θ2 = 3) are expected to select category 2 (Never). The compensatory nature of the MGRM is also shown in Figure 5.12: When individuals possess high levels of both θanx and θint, it is highly probable that they will select category 2 (Never).

# polytomous information

```{r}
for (i in 1:ncol(ques43)){ 

Ai <- sqrt(a1[i]^2+a2[i]^2)
info <- (Ai^2)*((z1*(1-z1))+(z2*(1-z2))+((z3*(1-z3))))
contour(x,y,info,levels=seq(0,max(info),.1),las=1,labcex=1,xlab=expression(theta[1]),cex.lab=1.5,ylab=expression(theta[2]),lwd=seq(0,5,.3))
}

```

Note that the z-axis is information I(θ) rather than probability. The wide white swath in the middle of the contour plot represents the plateau associated with θ values. Overall, then, this item provides accurate measurement of the person parameters across a considerable region of the θ-space. 




The Direction of Measurement
For the compensatory M2PL model, imagine that an item’s response surface is a piece of paper that has been bent into an S-shape (also known as an ogive). One latent trait may have a greater impact on the probability of a correct response to this particular item, so this S-shaped paper needs to be rotated accordingly. For instance, if θ1 plays a larger role than θ2 in the probability of correct response, then the paper should be rotated toward θ1. Consider the four contour plots shown in Figure 5.5. As discussed in the previous chapter, you can think of these contour plots as the view you would have when looking down through the surface onto the θ-plane. Here, we add three graphical elements that will assist us in comprehending the shape and orientation of the response surface. The first is a bolded contour line, which represents the combinations of θ1 and θ2 that are associated with a P = 0.5 probability of correct response (i.e., the same threshold used in defining the difficulty parameter in the unidimensional 2PL model presented in Chapter 2). The second is a dotted line indicating the axis along which the surface is oriented, which Muraki and Carlson (1995) termed the direction of measurement. The third is an arrow, pointing in the direction of measurement, whose length reflects the steepness of the surface such that longer arrows are indicative of steeper surfaces. We will return to slope steepness later.

The four contours in Figure 5.5 reflect items with equal difficulty. In each plot, the first discrimination parameter a1 is fixed at 1.0, but a2 varies. Plot (i) presents the contours when a2 = 0.1. Here, the second dimension hardly discriminates at all and thus barely plays a role in the probability of 

page 59 in book. use this to interpret. 



```{r}
MDisc <- data.frame(MDISC(mod_43_graded2))
# simply this is squaring each discrimination parameter and summing them up 

MDisc <- rownames_to_column(MDisc)

Mdiff <- data.frame(MDIFF(mod_43_graded2))

# here we divide each intercept by the MDISC 

Mdiff <- rownames_to_column(Mdiff)


```

Conceptually, the multidimensional Ai parameter is analogous to the unidimensional a parameter and can be interpreted similarly: Higher Ai values indicate an item response surface that discriminates well between low and high levels of the latent traits.

We can also compute a multidimensional difficulty index (sometimes referred to as MDIFF). In the 2-dimensional example, the origin of the θ-space represents average difficulty/ability with regard to both latent traits. 

The multidimensional Bi parameter is interpreted just as the unidimensional bi parameter: Items with high values of Bi require high levels of θ in order to have a high likelihood of correct response. Again, however, this interpretation is only accurate in a specific direction.

for the difficulty, I can sort them 


```{r}
Mdiff %>% 
  arrange(desc(MDIFF_2)) %>% 
  head
```

These are the most difficult questions that require a higher latent trait to select the choice "very common"


# conditional response functions


Finally, the bottom panel shows the θ1 response curve when the respondent’s ability with regard to θ2 is –2. We see that when θ2 is low, θ1 must be fairly high in order for the respondent to have a high correct response probability. Overall, we can think of the three conditional item response functions in terms of difficulty: As θ2 increases from –2 to 0 to 2, it becomes “easier” (in terms of θ1) for an examinee to provide a correct response. We could also slice the surface in the other direction so that the response probability is represented as a function of θ2 conditional on θ1. Doing so would reveal that a correct response is more likely when the respondent possesses strong abilities on either θ1 or θ2. (This relationship between dimensions demonstrates the compensatory nature of the M2PL: Low levels of θ2 can be compensated with high levels of θ1 and vice versa.)

```{r}



# z2 <- p1.grm(a1,a2,b11,b12,b21,b22)
# z3 <- p2.grm(a1,a2,b21,b22)


# p0.grm <- function(a1,a2,b11,b12) {
#   prob <- matrix(0,length(theta1),ncol(ques43))
#   for (i in 1:ncol(ques43)) {
#       z <- a1[i]*(theta1-b11[i])+a2[i]*(theta2_slice-b12[i])
#       p0star <- rep(1, length(theta1))
#       p1star <- exp(z)/(1+exp(z))
#       prob[,i] <- p0star-p1star
#   }
#   prob
# }
# 
# prob1 <- p0.grm(a1,a2,b11,b12)
# 
# # theta = 2 
# 
# theta2_slice <- 2 #modify value of 2nd dimension
# for (i in 1: ncol(ques43)){
#   plot(theta2,prob1[,i],type='l',ylim=c(0,1),las=1,xlim=c(-4,4),ylab='P(x = 1)',xlab=expression(theta[1]),lwd=3)
#   key <- as.expression(bquote(paste(theta[1],' | ',theta[2])==.(theta2_slice)))
#   legend("bottomright",legend=key,cex=1.5,bty='n')
# }
# 
# # theta = 0
# theta2_slice <- 0 #modify value of 2nd dimension
# for (i in 1: ncol(ques43)){
#   plot(theta2,prob1[,i],type='l',ylim=c(0,1),las=1,xlim=c(-4,4),ylab='P(x = 1)',xlab=expression(theta[1]),lwd=3)
#   key <- as.expression(bquote(paste(theta[1],' | ',theta[2])==.(theta2_slice)))
#   legend("bottomright",legend=key,cex=1.5,bty='n')
# }
# 
# # theta = -2 
# theta2_slice <- -2 #modify value of 2nd dimension
# for (i in 1: ncol(ques43)){
#   plot(theta2,prob1[,i],type='l',ylim=c(0,1),las=1,xlim=c(-4,4),ylab='P(x = 1)',xlab=expression(theta[1]),lwd=3)
#   key <- as.expression(bquote(paste(theta[1],' | ',theta[2])==.(theta2_slice)))
#   legend("bottomright",legend=key,cex=1.5,bty='n')
# }
# ```
# 
# # constraining theta 1 
# 
# 
# ```{r}
# 
# # theta 1 = 2
# 
# theta1_slice <- 2 #modify value of 2nd dimension
# for (i in 1: ncol(ques43)){
#   plot(theta1,prob1[,i],type='l',ylim=c(0,1),las=1,xlim=c(-4,4),ylab='P(x = 1)',xlab=expression(theta[2]),lwd=3)
#   key <- as.expression(bquote(paste(theta[2],' | ',theta[1])==.(theta1_slice)))
#   legend("bottomright",legend=key,cex=1.5,bty='n')
# }
# 
# 
# # theta 1 = 0
# 
# theta1_slice <- 0 #modify value of 2nd dimension
# for (i in 1: ncol(ques43)){
#   plot(theta1,prob1[,i],type='l',ylim=c(0,1),las=1,xlim=c(-4,4),ylab='P(x = 1)',xlab=expression(theta[2]),lwd=3)
#   key <- as.expression(bquote(paste(theta[2],' | ',theta[1])==.(theta1_slice)))
#   legend("bottomright",legend=key,cex=1.5,bty='n')
# }
# 
# # theta 1 = -2
# 
# theta1_slice <- -2 #modify value of 2nd dimension
# for (i in 1: ncol(ques43)){
#   plot(theta1,prob1[,i],type='l',ylim=c(0,1),las=1,xlim=c(-4,4),ylab='P(x = 1)',xlab=expression(theta[2]),lwd=3)
#   key <- as.expression(bquote(paste(theta[2],' | ',theta[1])==.(theta1_slice)))
#   legend("bottomright",legend=key,cex=1.5,bty='n')
# }
# 
# ```
# 
# 
# # for the second option 
# 
# ```{r}
# 
# p1.grm <- function(a1,a2,b11,b12,b21,b22) {
#   prob <- matrix(0,length(theta1),ncol(ques43))
#   for (i in 1:ncol(ques43)) {
#       z0 <- a1[i]*(theta1-b11[i])+a2[i]*(theta2_slice-b12[i])
#       z1 <- a1[i]*(theta1-b21[i])+a2[i]*(theta2_slice-b22[i])
#       p0star <- exp(z0)/(1+exp(z0))
#       p1star <- exp(z1)/(1+exp(z1))
#       prob[,i] <- p0star-p1star
#   }
#   prob
# }
# 
# prob2 <- p1.grm(a1,a2,b11,b12,b21,b22)
# 
# # theta = 2 
# 
# theta2_slice <- 2 #modify value of 2nd dimension
# for (i in 1: ncol(ques43)){
#   plot(theta2,prob2[,i],type='l',ylim=c(0,1),las=1,xlim=c(-4,4),ylab='P(x = 1)',xlab=expression(theta[1]),lwd=3)
#   key <- as.expression(bquote(paste(theta[1],' | ',theta[2])==.(theta2_slice)))
#   legend("bottomright",legend=key,cex=1.5,bty='n')
# }
# 
# # theta = 0
# theta2_slice <- 0 #modify value of 2nd dimension
# for (i in 1: ncol(ques43)){
#   plot(theta2,prob2[,i],type='l',ylim=c(0,1),las=1,xlim=c(-4,4),ylab='P(x = 1)',xlab=expression(theta[1]),lwd=3)
#   key <- as.expression(bquote(paste(theta[1],' | ',theta[2])==.(theta2_slice)))
#   legend("bottomright",legend=key,cex=1.5,bty='n')
# }
# 
# # theta = -2 
# theta2_slice <- -2 #modify value of 2nd dimension
# for (i in 1: ncol(ques43)){
#   plot(theta2,prob2[,i],type='l',ylim=c(0,1),las=1,xlim=c(-4,4),ylab='P(x = 1)',xlab=expression(theta[1]),lwd=3)
#   key <- as.expression(bquote(paste(theta[1],' | ',theta[2])==.(theta2_slice)))
#   legend("bottomright",legend=key,cex=1.5,bty='n')
# }
# 
# ```
# 
# # constraining theta 1 
# 
# ```{r}
# 
# # theta 1 = 2
# 
# theta1_slice <- 2 #modify value of 2nd dimension
# for (i in 1: ncol(ques43)){
#   plot(theta1,prob2[,i],type='l',ylim=c(0,1),las=1,xlim=c(-4,4),ylab='P(x = 1)',xlab=expression(theta[2]),lwd=3)
#   key <- as.expression(bquote(paste(theta[2],' | ',theta[1])==.(theta1_slice)))
#   legend("bottomright",legend=key,cex=1.5,bty='n')
# }
# 
# 
# # theta 1 = 0
# 
# theta1_slice <- 0 #modify value of 2nd dimension
# for (i in 1: ncol(ques43)){
#   plot(theta1,prob2[,i],type='l',ylim=c(0,1),las=1,xlim=c(-4,4),ylab='P(x = 1)',xlab=expression(theta[2]),lwd=3)
#   key <- as.expression(bquote(paste(theta[2],' | ',theta[1])==.(theta1_slice)))
#   legend("bottomright",legend=key,cex=1.5,bty='n')
# }
# 
# # theta 1 = -2
# 
# theta1_slice <- -2 #modify value of 2nd dimension
# for (i in 1: ncol(ques43)){
#   plot(theta1,prob2[,i],type='l',ylim=c(0,1),las=1,xlim=c(-4,4),ylab='P(x = 1)',xlab=expression(theta[2]),lwd=3)
#   key <- as.expression(bquote(paste(theta[2],' | ',theta[1])==.(theta1_slice)))
#   legend("bottomright",legend=key,cex=1.5,bty='n')
# }
# 
# ```
# 
# 
# # for the third option 
# 
# ```{r}
# 
# # p2.grm <- function(a1,a2,b21,b22, no) {
# #   prob <- matrix(0,length(theta1),length(theta2))
# #   for (i in 1:length(theta1)) {
# #     for (j in 1:length(theta2)) {
# #       z1 <- a1*(theta1[i]-b21)+a2*(theta2[j]-b22)
# #       p1star <- exp(z1)/(1+exp(z1))
# #       p2star <- rep(0, ncol(ques43))
# #       prob[i,j] <- p1star[no]-p2star[no]
# #     }
# #   }
# #   prob
# # }
# 
# p2.grm <- function(a1,a2,b21,b22) {
#   prob <- matrix(0,length(theta1),ncol(ques43))
#   for (i in 1:ncol(ques43)) {
#       z1 <- a1[i]*(theta1-b21[i])+a2[i]*(theta2-b22[i])
#       p1star <- exp(z1)/(1+exp(z1))
#       p2star <- rep(0, ncol(ques43))
#       # p0star <- rep(1, length(theta1))
#       # p1star <- exp(z)/(1+exp(z))
#       prob[,i] <- p0star-p1star
#   }
#   prob
# }
# 
# prob3 <- p2.grm(a1,a2,b21,b22)
# 
# # theta = 2 
# 
# theta2_slice <- 2 #modify value of 2nd dimension
# for (i in 1: ncol(ques43)){
#   plot(theta2,prob3[,i],type='l',ylim=c(0,1),las=1,xlim=c(-4,4),ylab='P(x = 1)',xlab=expression(theta[1]),lwd=3)
#   key <- as.expression(bquote(paste(theta[1],' | ',theta[2])==.(theta2_slice)))
#   legend("bottomright",legend=key,cex=1.5,bty='n')
# }
# 
# # theta = 0
# theta2_slice <- 0 #modify value of 2nd dimension
# for (i in 1: ncol(ques43)){
#   plot(theta2,prob3[,i],type='l',ylim=c(0,1),las=1,xlim=c(-4,4),ylab='P(x = 1)',xlab=expression(theta[1]),lwd=3)
#   key <- as.expression(bquote(paste(theta[1],' | ',theta[2])==.(theta2_slice)))
#   legend("bottomright",legend=key,cex=1.5,bty='n')
# }
# 
# # theta = -2 
# theta2_slice <- -2 #modify value of 2nd dimension
# for (i in 1: ncol(ques43)){
#   plot(theta2,prob3[,i],type='l',ylim=c(0,1),las=1,xlim=c(-4,4),ylab='P(x = 1)',xlab=expression(theta[1]),lwd=3)
#   key <- as.expression(bquote(paste(theta[1],' | ',theta[2])==.(theta2_slice)))
#   legend("bottomright",legend=key,cex=1.5,bty='n')
# }
# 
# ```
# 
# # contraining theta 1 
# 
# ```{r}
# 
# # theta 1 = 2
# 
# theta1_slice <- 2 #modify value of 2nd dimension
# for (i in 1: ncol(ques43)){
#   plot(theta1,prob3[,i],type='l',ylim=c(0,1),las=1,xlim=c(-4,4),ylab='P(x = 1)',xlab=expression(theta[2]),lwd=3)
#   key <- as.expression(bquote(paste(theta[2],' | ',theta[1])==.(theta1_slice)))
#   legend("bottomright",legend=key,cex=1.5,bty='n')
# }
# 
# 
# # theta 1 = 0
# 
# theta1_slice <- 0 #modify value of 2nd dimension
# for (i in 1: ncol(ques43)){
#   plot(theta1,prob3[,i],type='l',ylim=c(0,1),las=1,xlim=c(-4,4),ylab='P(x = 1)',xlab=expression(theta[2]),lwd=3)
#   key <- as.expression(bquote(paste(theta[2],' | ',theta[1])==.(theta1_slice)))
#   legend("bottomright",legend=key,cex=1.5,bty='n')
# }
# 
# # theta 1 = -2
# 
# theta1_slice <- -2 #modify value of 2nd dimension
# for (i in 1: ncol(ques43)){
#   plot(theta1,prob3[,i],type='l',ylim=c(0,1),las=1,xlim=c(-4,4),ylab='P(x = 1)',xlab=expression(theta[2]),lwd=3)
#   key <- as.expression(bquote(paste(theta[2],' | ',theta[1])==.(theta1_slice)))
#   legend("bottomright",legend=key,cex=1.5,bty='n')
# }
# 
# ```
# 
# 
# ```{r}
# 
# 
# p0.grm <- function(a1,a2,b11,b12) {
#   prob <- matrix(0,length(theta1),ncol(ques43))
#   for (i in 1:ncol(ques43)) {
#       z <- a1[i]*(theta1-b11[i])+a2[i]*(theta2_slice-b12[i])
#       p0star <- rep(1, length(theta1))
#       p1star <- exp(z)/(1+exp(z))
#       prob[,i] <- p0star-p1star
#   }
#   prob
# }
# 
# prob1 <- p0.grm(a1,a2,b11,b12)
# 
# theta2_slice <- 2 #modify value of 2nd dimension
# for (i in 1: ncol(ques43)){
#   plot(theta2,prob1[,i],type='l',ylim=c(0,1),las=1,xlim=c(-4,4),ylab='P(x = 1)',xlab=expression(theta[1]),lwd=3)
#   key <- as.expression(bquote(paste(theta[1],' | ',theta[2])==.(theta2_slice)))
#   legend("bottomright",legend=key,cex=1.5,bty='n')
# }
# 
# ```
# theta2_slice <- 2 #modify value of 2nd dimension
# for (i in 1:ncol(ques43)){
#   plot(theta2, p0.grm(a1,a2,b11,b12,i),type='l',ylim=c(0,1),las=1,xlim=c(-4,4),ylab='P(x =1)',xlab=expression(theta[1]),lwd=3)
# }
# key <- as.expression(bquote(paste(theta[1],' | ',theta[2])==.(theta2_slice)))
# legend("bottomright",legend=key,cex=1.5,bty='n')
# 
# c <- p0.grm(a1,a2,b11,b12, 1)
# 
# for (i in 1:ncol(ques43)){
#   plot(theta2, p0.grm(a1,a2,b11,b12,i),type='l',ylim=c(0,1),las=1,xlim=c(-4,4),ylab='P(x =1)',xlab=expression(theta[1]),lwd=3)
# }
# 
# 
# ```
# 
# 
# 
# 
# 
# for (i in 1:ncol(ques43)){
#   contour(x,y, p1.grm(a1,a2,b11,b12,b21,b22,i),las=1,labcex=1,xlab=expression(theta[1]),cex.lab=1.5,ylab=expression(theta[2]))
# }
# 
# for (i in 1:ncol(ques43)){
#   contour(x,y,p2.grm(a1,a2,b21,b22,i),las=1,labcex=1,xlab=expression(theta[1]),cex.lab=1.5,ylab=expression(theta[2]))
# }
# 
# 
# ```

```


# ```{r}
# coef(mod_51_graded3)
# 
# ```
# 
# ### multidimentional item location 
# 
# ```{r}
# head(MDIFF(mod_51_graded3))
# 
# ```
# 
# ### person parameters 
# 
# ```{r}
# 
# head(fscores(mod_51_graded3))
# rel_49 <- head(fscores(mod_49_graded3, method = "EAP", returnER = T))
# rel_51 <- head(fscores(mod_51_graded3, method = "EAP", returnER = T))  # emperical reliability
# 
# 
# 
# 
# rel.mirt <- function(x){
#   eap <- mirt::fscores(x, full.scores = T, scores.only = T, full.scores.SE = T)
#   e <- mean(eap[, 2]^2)
#   s <- var(eap[, 1])
#   1-(e/(s+e))
# }
# 
# rel.mirt(mod_49_graded3)
# 
# ```
# 
# ### residuals
# 
# ```{r}
# 
# res <- residuals(mod_51_graded3, type = "LDG2", digits = 4, df.p = T)
# 
# ```
# 
# # exploratory multigroup MIRT
# 
# ### for parent 
# 
# ```{r}
# 
# class2 <- SurveyData$parent
# levels(class2)
# modMG <- multipleGroup(ques53, model = 2, group = class2,
# SE = TRUE, itemtype = "graded", TOL = 0.01)  # model =2 means two dimentional model
# # https://groups.google.com/forum/#!searchin/mirt-package/multiplegroup$20exploratory|sort:date/mirt-package/j-ZyjuXPNcY/zoSTk-BEBgAJ 
# 
# # which p-value to use ?  Ans: 0.05, but u can use 0.01 
# # https://groups.google.com/forum/#!searchin/mirt-package/multiplegroup$20exploratory|sort:date/mirt-package/CXWkXsKCBvo/WAivQ5ABBQAJ
# 
# astiDIF <- DIF(modMG, c('a1', 'd'), Wald = TRUE,
# p.adjust = 'fdr')
# round(astiDIF$adj_pvals[astiDIF$adj_pvals < 0.05], 4)
# ```
# # try with the 43 questions
# 
# ```{r}
# modMG_43 <- multipleGroup(ques43, model = 2, group = class2,
# SE = TRUE, itemtype = "graded", TOL = 0.01)  # model =2 means two dimentional model
# 
# astiDIF_43 <- DIF(modMG_43, c('a1', 'd'), Wald = TRUE,
# p.adjust = 'fdr')
# round(astiDIF_43$adj_pvals[astiDIF_43$adj_pvals < 0.05], 4)
# 
# ```
# 
# no DIF items 
# 
# ### parent education 
# 
# ```{r}
# 
# class3 <- SurveyData$parent_education
# levels(class3) <- c("ILL", "rwp", "psec", "uni")
# 
# modMG_edu <- multipleGroup(ques53, model = 2, group = class3, itemtype = "graded", TOL = 0.01, SE = T)  
# 
# DIF_edu <- DIF(modMG_edu, c('a1', 'd'), Wald = T,
# p.adjust = 'fdr', simplify = T, verbose = T)
# round(DIF_edu$adj_pvals[astiDIF$adj_pvals < 0.05], 4)
# 
# 
# 
# ```
# 
# ### income category 
# 
# ```{r}
# 
# class4 <- factor(SurveyData$income_category)
# modMG_inc <- multipleGroup(ques53,  model = 2, group = class4, itemtype = "graded", TOL = 0.01, SE= T)
# 
# DIF_inc <- DIF(modMG_inc, c('a1', 'd'), Wald = TRUE,
# p.adjust = 'fdr')
# round(DIF_inc$adj_pvals[astiDIF$adj_pvals < 0.05], 4)
# 
# 
# ```
# 
# 
# ### Number of children 
# 
# ```{r}
# 
# class5 <- factor(SurveyData$Number_of_family_members)
# modMG_mem <- multipleGroup(ques53,  model = 2, group = class5, itemtype = "graded", TOL = 0.01, SE= T)
# 
# DIF_mem <- DIF(modMG_mem, c('a1', 'd'), Wald = TRUE,
# p.adjust = 'fdr')
# round(DIF_mem$adj_pvals[astiDIF$adj_pvals < 0.05], 4)
# 
# ```
# 
# 
# 
# # TRY after removal 2 questions
# 
# ```{r}
# ques43_irt <- ques53 %>% 
#   dplyr::select(- c(40,44))
# 
# mod_51_graded3_43 <- mirt(ques43_irt, 2, itemtype = "graded")
# 
# stat_graded_43 <- M2(mod_51_graded3_43) # didn't use type C2 since I have too many dfs 
# 
```
 
# Try after removal 20 questions 

```{r}
# ques19_irt <- ques53 %>%
#   dplyr::select(- c(20:61))
# 
# mod_51_graded3_19 <- mirt(ques19_irt, 1, itemtype = "graded")
# 
# stat_graded_19 <- M2(mod_51_graded3_19) # didn't use type C2 since I have too many dfs

```

```{r}
# 
# plot(mod_graded3, type = "info", main = "Item Information")
# 
# # This plot (not shown here) tells us in which trait area our entire scale is
# # informative and thus able to assess a person’s location on the conservatism trait
# # with good precision

```


### Limitations

We have used only three levels of likert scale questions. Results could have been more powerful if we used at least five levels. In addition, using an odd number of levels may drive the responder to choose the middle response. On the contrary, using an even number of levels could have avoided this. 





# Supplementary figures 

### likert plot for demand factor

```{r fig.height= 15, fig.width= 15}
#https://cran.r-project.org/web/packages/HH/HH.pdf

lkrt_plot <- function(factor, name){
  
  fct <- data.frame(cbind(SurveyData[1], factor ))
  
  fct <- gather(fct, "question", "response", -ID)
  
  g <- fct %>% group_by(question, response) %>% 
    summarise(n = n()) %>% 
    arrange(response)
  
  g <- reshape2::melt(g, id.vars=c("question", "response"))
  names(g)[3] <- "Agreement"
  
  g$response <- as.factor(g$response)
  
  levels(g$response) <- c("never", "sometimes", "common")
  
  return(likert(question ~ response , value="value", data=g,
         main = name,
         as.percent = T,
         ylab=NULL,
         scales=list(y=list(relation="free")), layout=c(1,2)))
  
}


```


We checked whether irrational believes scores vary according to parent or child factors. We applied multivariate analysis of variance instead of univariate ANOVA or t-test to avoid family-wise error rate(alpha inflation). In addition, this enables us to preserve power since scale scores are correlated. 



Parametric tests as MANOVA should be used with caution with ordinal data like the likert scale we are dealing with, as assumptions maybe violated. However, we are using the average score for the questions. This renders the questions in a continuous rather than a categorical form. The same methodology has been previously applied in a similar article written by [@bonnerDevelopmentValidationParent2006]. Moreover, with the large sample size we have, normality is never a problem. [click to view reference](https://www.st-andrews.ac.uk/media/capod/students/mathssupport/OrdinalexampleR.pdf). Box's M test was used to test the assumption of homogeneity of variances and covariances and the assumption was not violated.  

```{r}

# MANOVA for gender

# Assumptions of MANOVA

# MANOVA can be used in certain conditions:
# 
# The dependent variables should be normally distribute within groups. The R function mshapiro.test( )[in the mvnormtest package] can be used to perform the Shapiro-Wilk test for multivariate normality. This is useful in the case of MANOVA, which assumes multivariate normality.
# 
# Homogeneity of variances across the range of predictors.
# 
# Linearity between all pairs of dependent variables, all pairs of covariates, and all dependent variable-covariate pairs in each cell

library(RVAideMemoire)

#mshapiro.test(cor_data)

# here I tried to test for multivariate normality, however, due to the large sample size, p value of shapiro wilk test is definetely significant. Thus, we will shift to the package MVN: An R Package for Assessing Multivariate Normality  

library(MVN)

# result <- mvn(data = cor_data, mvnTest = "mardia")
# result$multivariateNormality
# 
# 
# result <- mvn(data = cor_data, mvnTest = "hz")
# result$multivariateNormality
# 
# 
# result <- mvn(data = cor_data, mvnTest = "royston")
# result$multivariateNormality
# # this depends on shapiro, so shouldn't be used with a large sample size
# 
# 
# result <- mvn(data = cor_data, mvnTest = "dh")
# result$multivariateNormality
# 
# 
# result <- mvn(data = cor_data, mvnTest = "energy")
# result$multivariateNormality

# here it appears that for all tests, the normality assumption is violated, however, this is not a problem because we have a large sample size. 


# mnva_qplot <- function(data){
#   # create univariate Q-Q plots
# mvn(data = data, mvnTest = "royston", univariatePlot = "qqplot")
# }
# 
# dsc_table <- mnva_qplot(cor_data)
# 
# dsc_table <- dsc_table$Descriptives[1:8]
# 
# rmdtable(dsc_table)

# mnva_hist <- function(data){   # create univariate histograms
# 
#   mvn(data = cor_data, mvnTest = "royston", univariatePlot = "histogram")
# }
# 
# mnva_hist(cor_data)


# From the plots, it appears that demand factor is the most problematic. 
# However, again this is not a problem

# https://cran.r-project.org/web/packages/MVN/vignettes/MVN.pdf  the perfect package


# Now it is time to test for homogenity of variance assumption 

 # create a data set for three mult columns and sex 

test_data <- cbind(cor_data, SurveyData$sex)

colnames(test_data)[4] <- "sex"

library(micompr)

#assumptions_manova(cor_data, test_data$sex)

# Box's M test is a multivariate statistical test used to check the equality of multiple variance-covariance matrices.[1] The test is commonly used to test the assumption of homogeneity of variances and covariances in MANOVA and linear discriminant analysis. It is named after George E. P. Box.
# 
# Box's M test is susceptible to errors if the data does not meet model assumptions or if the sample size is too large or small.[2] Box's M test is especially prone to error if the data does not meet the assumption of multivariate normality


# here it appears that multivariate homogenity of variance assumption is not violated


################

# datacamp made a quick reference for MANOVA with assumptions 

# I will move with this guide step by step 

# https://www.statmethods.net/stats/anova.html 

mnv_tbl <- function(col){
  
  fit_sex <- tidy(manova(as.matrix(cor_data) ~ col), test = "Hotelling-Lawley")

fit_sex <- fit_sex[1, c(1,2,3,4,7)]

colnames(fit_sex) <- c("Factor", "df", "Hotelling-Lawley trace", "F", "p.value")

return(fit_sex)  
}

mnv_sex <- mnv_tbl(SurveyData$sex)

# fit_sex <- tidy(manova(as.matrix(cor_data) ~ SurveyData$sex, test = "Hotelling-Lawley"), test = "Hotelling-Lawley")
# 
# fit_sex <- fit_sex[1, c(1,2,3,4,7)]
# 
# colnames(fit_sex) <- c("Factor", "df", "Hotelling-Lawley trace", "F", "p.value")


# for gender, we have 1 degree of freedom, so all tests should be identical. 
# however, here wilks is different and I don't know why 

# summary(fit, test="Pillai")
# 
# summary(fit, test="Wilks")
# 
# summary(fit, test="Hotelling-Lawley")
# 
# summary(fit, test="Roy")


# When the hypothesis degrees of freedom, h, is one, all four test statistics will lead to identical results. When h>1,
# the four statistics will usually lead to the same result. When they do not, the following guidelines from
# Tabachnick (1989) may be of some help.
# Wilks' Lambda, Lawley's trace, and Roy's largest root are often more powerful than Pillai's trace if h>1 and one
# dimension accounts for most of the separation among groups. Pillai's trace is more robust to departures from
# assumptions than the other three.
# Tabachnick (1989) provides the following checklist for conducting a MANOVA. We suggest that you consider
# these issues and guidelines carefully.

# significant result, and it is the same value for all tests

#summary.aov(fit, test="Wilks")  # univariate anova

# checking assumptions

# The aq.plot() function in the mvoutlier package allows you to identfy multivariate outliers by plotting the ordered squared robust Mahalanobis distances of the observations against the empirical distribution function of the MD2i. Input consists of a matrix or data frame. The function produces 4 graphs and returns a boolean vector identifying the outliers.
library(mvoutlier)

# outliers <- aq.plot(cor_data)
# outliers # show list of outliers

# testing normality 

# mshapiro.test(as.matrix(cor_data))
# 
# # Graphical Assessment of Multivariate Normality
# x <- as.matrix(cor_data) # n x p numeric matrix
# center <- colMeans(x) # centroid
# n <- nrow(x); p <- ncol(x); cov <- cov(x); 
# d <- mahalanobis(x,center,cov) # distances 
# qqplot(qchisq(ppoints(n),df=p),d,
#   main="QQ Plot Assessing Multivariate Normality",
#   ylab="Mahalanobis D2")
#abline(a=0,b=1)

```



```{r}

# sex_manova <- rmdtable(tidy(HotellingsT2(as.matrix(cor_data) ~ SurveyData$sex)))

# SPSS will give Hotelling's Trace, and it has to convert to Hotelling's T^2 as follows:
#   Multiplying Hotelling's Trace by (N - L), where N is the sample size across all groups and L is the number of groups, gives a generalized version of Hotelling's T^2.

# https://www.researchgate.net/post/How_can_I_do_Hotellings_T-square 

#Parent score differ according to child's gender. 

```



```{r}

### MANOVA father education 

# SurveyData$mo_edu <- SurveyData$Mother_education
# 
# SurveyData$mo_edu <- car::recode(SurveyData$mo_edu, "'Illiterate' = 1; c('read n write', 'primary') = 2; 
#   c('prep', 'secondary +') = 3; 'University Degree' = 4")
# 
# 
# SurveyData$fa_edu <- car::recode(SurveyData$father_edu_combined, "'Illiterate' = 1; 'R&W_prim' = 2; 'prep/sec' = 3; 'University Degree & post-grad' = 4")
# 
# SurveyData$parent_education <- SurveyData$fa_edu
# 
# SurveyData$parent_education[SurveyData$author == "mother"] <- SurveyData$mo_edu[SurveyData$author == "mother"]
# 
# levels(SurveyData$parent_education) <- c("Illiterate", "R&W-PRIM", "PREP-SEC", "University")
# 
edu_data <- cbind(cor_data, SurveyData$parent_education)

colnames(edu_data)[4] <- "edu"

edu_data <- edu_data[complete.cases(edu_data),]

#assumptions_manova(edu_data[1:3], as.factor(edu_data$edu))

# here the homogenity of variance assumption is not violated
#although the test is sensitive to large numbers, meaning that it may cause the assumption to be violated even when tiny departures from hokmogenity are present. 
#however, here it is not violated.

fit_edu <- manova(as.matrix(cor_data) ~ SurveyData$parent_education)

m <- list(summary(fit_edu, test="Pillai"),
summary(fit_edu, test="Wilks"),
summary(fit_edu, test="Hotelling-Lawley"),
summary(fit_edu, test="Roy"),
summary.aov(fit_edu, test="Wilks")  # univariate anova
)

mnv_edu <- mnv_tbl(SurveyData$parent_education)

```

```{r}

### MANOVA for income

inc_data <- cbind(cor_data, SurveyData$income_category_comb)

colnames(inc_data)[4] <- "inc"
inc_data <- inc_data[complete.cases(inc_data),]

#assumptions_manova(inc_data[1:3], as.factor(inc_data$inc))

# homogenity of variance assumption is not violated

mnv_income <- mnv_tbl(SurveyData$income_category_comb)

fit <- manova(as.matrix(inc_data[1:3]) ~ inc_data$inc)
#tidy(fit, test = "Hotelling-Lawley")

m <- list(summary(fit, test="Pillai"),
summary(fit, test="Wilks"),
summary(fit, test="Hotelling-Lawley"),
summary(fit, test="Roy"),
summary.aov(fit, test="Wilks")  # univariate anova
)

 # it is highly significant for all

```


```{r}

# ANOVA parent gender

parent_data <- cbind(cor_data, SurveyData$author)

colnames(parent_data)[4] <- "parent"

parent_data <- parent_data[complete.cases(parent_data),]

#assumptions_manova(parent_data[1:3], as.factor(parent_data$parent))

#assumption not working and I don't know why

#assumptions_manova(parent_data[1:3], parent_data$parent)

# homogenity of variance assumption is not violated

mnv_parent <- mnv_tbl(SurveyData$parent_education)
# results of the function are wrong 

fit_parent <- manova(as.matrix(parent_data[1:3]) ~ parent_data$parent)

fit_parent <- tidy(fit_parent, test = "Hotelling-Lawley")

m <- list(summary(fit, test="Pillai"),
summary(fit, test="Wilks"),
summary(fit, test="Hotelling-Lawley"),
summary(fit, test="Roy"),
summary.aov(fit, test="Wilks")  # univariate anova
)

fit_parent <- fit_parent[1, c(1,2,3,4,7)]

colnames(fit_parent) <- c("Factor", "df", "Hotelling-Lawley trace", "F", "p.value")

mnv_tbls <- rbind(mnv_sex, fit_parent, mnv_edu, mnv_income )
 # it is highly significant for all

mnv_tbls$Factor <- c("child gender", "parent gender", "parent education", "family income")

mnv_tbls <- rmdtable(mnv_tbls)

mnv_tbls <- color(mnv_tbls, i = ~ p.value < 0.05, j = ~ p.value , color = "red")
mnv_tbls
```
Table MANOVA results for parent and children factors

The scale scores varied significantly. with child gender, family income and parent education but not with parent gender. For child gender and parent gender, the degrees of freedom were 1. For all results, the four tests, Pillai, Wilks, Hotelling-Lawley and Roy provided quite similar results and here we report Hotelling-Lawley trace results. 


### Post-hoc MANOVA family income


```{r}

levels(inc_data$inc) <- c("<2", "2-6", "6-12", ">12")

inc_post <- mergeFactors(response = inc_data[,1:3],
                                   factor = factor(inc_data$inc),
                                   method = "adaptive") 

plot(inc_post, panel = "GIC", nodesSpacing = "effects", colorCluster = TRUE)

```

The higher the family income, the less common irrational believes are. 

### Domains' Averages according to income group 


```{r}

inc_mean <- inc_data %>% 
  group_by(inc) %>%
  summarise_all(mean)

rmdtable(inc_mean)
```


### Significance of groups splitting 


```{r}

inc_post_h <- mergingHistory(inc_post, showStats = TRUE) 
inc_post_h <- rmdtable(inc_post_h)

inc_post_h <- color(inc_post_h, i = ~ pvalVsPrevious < 0.05, j = ~ pvalVsPrevious , color = "red")

inc_post_h

```

Each row of the above frame describes one step of the merging algorithm. First two columns specify which groups were merged in the iteration. Last two columns are p-values for the Likelihood Ratio Test - against the full model (pvalVsFull) and against the previous one (pvalVsPrevious). Only the last step is significant, splitting the income categories into less than 6000 and more than 6000. 


### Post hoc MANOVA parent education level


```{r fig.height= 12, fig.width= 12}

library(factorMerger)

# post hoc MANOVA education

edu_post <- mergeFactors(response = edu_data[,1:3],
                                   factor = factor(edu_data$edu),
                                   method = "adaptive") 

plot(edu_post, panel = "GIC", nodesSpacing = "effects", colorCluster = TRUE)

```

Parents with a university degree or higher experience less commonly irrational believes than others with lower educational level. 


### Domains' Averages according to income group 


```{r}

edu_mean <- edu_data %>% 
  group_by(edu) %>%
  summarise_all(mean)

rmdtable(edu_mean)

```

### Significance of groups splitting 


```{r}

edu_post_h <- mergingHistory(edu_post, showStats = TRUE) 
edu_post_h <- rmdtable(edu_post_h)

edu_post_h <- color(edu_post_h, i = ~ pvalVsPrevious < 0.05, j = ~ pvalVsPrevious , color = "red")

edu_post_h
# data_edu <- SurveyData

# data_edu <- filter(data_edu, !is.na(fa_edu), !is.na(mo_edu))
# 
# data_edu$max_edu <- pmin(data_edu$fa_edu, data_edu$mo_edu)
# 
# n <- data.frame(cbind(data_edu$fa_edu, data_edu$mo_edu)) # I am using this step, because pmax is producing wrong results in the large dataframe
# 
# n$max <- pmax(n$X1, n$X2)
# 
# data_edu$max_edu <- n$max
# # table(n$max)
# table(data_edu$max_edu)


```

Splitting university from all other educational levels yielded significant results. 


# Detailed statistical results



```{r}

# We found that score differs significantly for the total questions. However, bear is the only factor where sex is found to be significantly affecting the score. 


# demand_p <- var.test(demand ~ sex, data = dat.clean)
# 
# demand_p <- demand_p$p.value
# 
# bear_p <- var.test(bear ~ sex, data = dat.clean)
# bear_p <- bear_p$p.value
# 
# accuse_p <- var.test(accuse ~ sex, data = dat.clean)
# accuse_p <- accuse_p$p.value
# 
# cbind(demand_p, bear_p, accuse_p)
# # combining box plots 

# # ggboxplot(dat.clean, x = "father_edu_combined", y = "cognitive_dim", 
# #           color = "income_category_comb",
#           ylab = "Mean score", xlab = "Income category")

```


### detect redundant questions 

```{r}
all_q <- round(all_qu$rho, 2)  # 2 decimal places
all_q[all_q == 1] <- 0  # convert ones to zeros    
all_q <- data.frame(all_q)
max(all_q)

all_q

bes <- all_q %>%
  rownames_to_column('gene') %>%
  filter_if(is.numeric, any_vars(. >0.62)) %>%
  column_to_rownames('gene')   # get only questions that have correlation less than 0.3 with all other variables. 

# tau is The normal equivalent of the cutpoints

)
Phi two binary 

, L, C or lambda


if(!require(rcompanion)){install.packages("rcompanion")}
if(!require(psych)){install.packages("psych")}
if(!require(DescTools)){install.packages("DescTools")}

epsilonSquared(questions, group = "row")

Input =(
"Adopt      Always  Sometimes  Never
Size
Hobbiest         0          1      5
Mom-and-pop      2          3      4
Small            4          4      4
Medium           3          2      0
Large            2          0      0
")

Tabla = as.table(read.ftable(textConnection(Input)))

class(Tabla)

SomersDelta(as.data.table(questions)
,
            direction  = "column",
            conf.level = 0.95)


```

```{r}

for(i in 1:ncol(all_q)){
  print(c(i, max(all_q[i])))
}  # get the max correlation across a column 

# however, polychoric correlations are conservative 


trial <- questions %>% 
  dplyr::select(Q26, Q44)

asd <- polychoric(trial)

asd <- tetrachoric(trial)
?polychoric
sum(questions[26] == questions[44])

sp <- cor(questions, method = "spearman")  # 
sp <- round(sp, 2)  # 2 decimal places
sp[sp == 1] <- 0  # convert ones to zeros    
sp <- data.frame(sp)

kn <- cor(questions, method = "kendall")  # 
kn <- round(kn, 2)  # 2 decimal places
kn[kn == 1] <- 0  # convert ones to zeros    
kn <- data.frame(kn)


trial_all <- all_q %>%
  rownames_to_column('gene') %>%
  filter_if(is.numeric, all_vars(. < 0.3)) %>%
  column_to_rownames('gene')   # get only questions that have correlation less than 0.3 with all other variables. 


all_low <- row.names(trial_all) 

foo <- function(){
  a = {1}
for (i in 1:ncol(questions)){
  for (j in 1:ncol (questions)){
    a = c(a, sum(questions[i] == questions[j])) 
    
#return(sum(questions[i] == questions[j]))
  }}
  return (a)
}

a <- foo()

a[a ==950] <- 0

sort(a, decreasing = T)

#polychoric correlation produces more conservative results (produces lower absolute values).

itemfit(mod_43_graded2)


```


### References



